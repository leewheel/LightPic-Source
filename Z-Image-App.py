import os
import time


# å¿…é¡»å¤„äºæ–‡ä»¶æœ€é¡¶ç«¯ï¼šç¯å¢ƒé…ç½®
os.environ["DIFFUSERS_USE_PEFT_BACKEND"] = "1"
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"
os.environ["HF_HUB_OFFLINE"] = "1"


import sys
import torch
import psutil
import random
import re
import uuid
import gc
import tempfile
import json
import string
from datetime import datetime
from PIL import Image, ImageFilter, ImageOps, ImageEnhance, ImageDraw

# å¼ºåˆ¶ stdout/stderr ä½¿ç”¨ UTF-8
# sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
# sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# ==========================================
# æ–°å¢ï¼šå¯¼å…¥æ ‡å‡†é‡‡æ ·å™¨è°ƒåº¦å™¨
# ==========================================
from diffusers import (
    EulerAncestralDiscreteScheduler,
    EulerDiscreteScheduler,
    DPMSolverMultistepScheduler,
    DPMSolverSinglestepScheduler,
    DDIMScheduler,
    UniPCMultistepScheduler,
    EDMEulerScheduler  # å¯¼å…¥ä»¥ä¾¿æ£€æŸ¥ç±»å‹ï¼Œè™½ç„¶é»˜è®¤æ—¶æˆ‘ä»¬ä¸æ‰‹åŠ¨å®ä¾‹åŒ–å®ƒ
)

# å¯æ ¹æ®éœ€æ±‚å¢å‡é£æ ¼ï¼Œä½†ä¸æ¶‰åŠå…·ä½“å¯¹è±¡
STYLE_POOL = [
    "documentary photography",
    "natural history archival",
    "field biology specimen photography",
    "å“ºä¹³åŠ¨ç‰©æ‘„å½±",
    "é¸Ÿç±»æ‘„å½±",
    "æ˜†è™«æ‘„å½±",
    "æ¤ç‰©æ‘„å½±",
    "å¾®è·æ‘„å½±",
    "è‡ªç„¶é£å…‰æ‘„å½±",
    "ç”Ÿæ€æ‘„å½±",
    "å® ç‰©æ‘„å½±",
    "å›½å®¶åœ°ç†æ‚å¿—çºªå½•ç‰‡"
]

def augment_prompt(user_prompt: str) -> str:
    """
    åœ¨ç”¨æˆ·è‡ªç”±è¾“å…¥çš„ prompt ä¸Šå¢åŠ éšæœºæ‰°åŠ¨ï¼š
    - Specimen IDï¼šéšæœºå­—æ¯æ•°å­—
    - Photography styleï¼šéšæœºé€‰æ‹©
    """
    random_id = ''.join(random.choices(string.ascii_uppercase + string.digits, k=6))
    style = random.choice(STYLE_POOL)
    return f"{user_prompt}\nSpecimen ID: {random_id}\nPhotography style: {style}"

# é…ç½®åŸºç¡€è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

# ç›®å½•é…ç½®
DEFAULT_MODEL_PATH = os.path.join(current_dir, "ckpts", "Z-Image-Turbo")
LORA_ROOT = os.path.join(current_dir, "lora")
OUTPUT_ROOT = os.path.join(current_dir, "outputs")
MOD_VAE_DIR = os.path.join(current_dir, "Mod", "vae")
MOD_TRANS_DIR = os.path.join(current_dir, "Mod", "transformer")
for p in [LORA_ROOT, OUTPUT_ROOT, MOD_VAE_DIR, MOD_TRANS_DIR]:
    os.makedirs(p, exist_ok=True)

try:
    import gradio as gr
    from diffusers import ZImagePipeline, ZImageImg2ImgPipeline, AutoencoderKL
    from safetensors.torch import load_file
except ImportError as e:
    print(f"Core import failed: {e}")
    sys.exit(1)

# ==========================================
# å›½é™…åŒ–é…ç½® (I18N)
# ==========================================
I18N = {
    "zh": {
        "title": "# ğŸ¨ Z-Image-Turbo LowVram Edition",
        "btn_lang": "Language / è¯­è¨€",
        "tab_t2i": "æ–‡æˆå›¾",
        "tab_edit": "å›¾ç‰‡ç¼–è¾‘",
        "tab_i2i": "å›¾ç”Ÿå›¾",
        "tab_inpaint": "å±€éƒ¨é‡ç»˜",
        "tab_fusion": "èåˆå›¾",
        
        "label_prompt": "Prompt",
        "btn_flush_vram": "ğŸ§¹ æ¸…ç†æ˜¾å­˜",
        "label_vram_threshold": "è‡ªåŠ¨æ¸…ç†é˜ˆå€¼ (%)",
        
        "acc_lora": "LoRA æƒé‡è®¾ç½®",
        "btn_refresh_lora": "ğŸ”„ åˆ·æ–° LoRA æ–‡ä»¶åˆ—è¡¨",
        "txt_no_lora": "*æœªæ£€æµ‹åˆ° LoRA æ–‡ä»¶*",
        "label_weight": "æƒé‡",
        
        "acc_model": "æ¨¡å‹è®¾ç½®",
        "btn_refresh_model": "ğŸ”„ åˆ·æ–°åº•æ¨¡/VAE",
        "label_transformer": "Transformer",
        "label_vae": "VAE",
        "label_perf": "æ€§èƒ½æ¨¡å¼",
        "val_perf_high": "é«˜ç«¯æœº (æ˜¾å­˜>=20GB)",
        "val_perf_low": "ä½ç«¯æœº (æ˜¾å­˜ä¼˜åŒ–)",
        "label_width": "å®½ (16å€æ•°)",
        "label_height": "é«˜ (16å€æ•°)",
        "label_steps": "æ­¥æ•°",
        "label_cfg": "CFG",
        "label_batch": "ç”Ÿæˆå¼ æ•°",
        "label_seed": "ç§å­",
        "label_random_seed": "éšæœºç§å­",
        "label_sampler": "é‡‡æ ·å™¨",
        
        "btn_run": "ğŸš€ å¼€å§‹ç”Ÿæˆ",
        "btn_stop": "ğŸ›‘ åœæ­¢ç”Ÿæˆ",
        "btn_gen": "ğŸš€ ç”Ÿæˆ",
        "btn_stop_short": "ğŸ›‘ åœæ­¢",
        "label_output": "è¾“å‡ºç»“æœ",
        "label_gallery_i2i": "å›¾ç”Ÿå›¾ç»“æœ",
        "label_gallery_inpaint": "å±€éƒ¨é‡ç»˜ç»“æœ",
        "label_gallery_fusion": "èåˆç»“æœ",
        
        "label_upload_img": "ä¸Šä¼ å›¾ç‰‡",
        "label_rotate": "æ—‹è½¬è§’åº¦ (åº¦)",
        "label_crop_x": "è£å‰ª X (%)",
        "label_crop_y": "è£å‰ª Y (%)",
        "label_crop_w": "è£å‰ªå®½åº¦ (%)",
        "label_crop_h": "è£å‰ªé«˜åº¦ (%)",
        "label_flip_h": "æ°´å¹³ç¿»è½¬",
        "label_flip_v": "å‚ç›´ç¿»è½¬",
        "btn_edit": "å¼€å§‹ç¼–è¾‘",
        "label_edited": "ç¼–è¾‘åçš„å›¾ç‰‡",
        "label_filter": "åº”ç”¨æ»¤é•œ",
        "label_brightness": "äº®åº¦è°ƒæ•´ (%)",
        "label_contrast": "å¯¹æ¯”åº¦è°ƒæ•´ (%)",
        "label_saturation": "é¥±å’Œåº¦è°ƒæ•´ (%)",
        
        "label_ref_img": "ä¸Šä¼ å‚è€ƒå›¾",
        "label_prompt_rec": "Prompt (æ¨è)",
        "ph_prompt_i2i": "æè¿°ä½ æƒ³è¦ç”Ÿæˆçš„ç”»é¢...",
        "label_out_w": "è¾“å‡ºå®½ (0=è‡ªåŠ¨ä¿æŒæ¯”ä¾‹)",
        "label_out_h": "è¾“å‡ºé«˜ (0=è‡ªåŠ¨ä¿æŒæ¯”ä¾‹)",
        "tip_res": "**æç¤ºï¼š** å®½é«˜éƒ½ä¸º0æ—¶è‡ªåŠ¨ä¿æŒä¸Šä¼ å›¾æ¯”ä¾‹å¹¶æ¥è¿‘1024ï¼›æ‰‹åŠ¨è®¾ç½®å¤§äº512æ—¶ç”Ÿæ•ˆ",
        "label_strength": "é‡ç»˜å¼ºåº¦",
        "label_cfg_fixed": "CFGï¼ˆTurboæ¨¡å‹å›ºå®šä¸º0.0ï¼‰",
        "label_cfg_turbo": "CFG (Turbo Img2Img)",
        "label_cfg_inpaint": "CFG (Inpaint)",
        
        # --- Inpaint ç‰¹æœ‰ ---
        "lbl_inpaint_editor": "ç»˜åˆ¶ Mask (ç™½è‰²ä¸ºä¿®æ”¹åŒºï¼Œé»‘è‰²ä¸ºä¿ç•™åŒº)",
        "lbl_inpaint_tip": "æç¤ºï¼šå…ˆåœ¨ä¸Šæ–¹'ä¸Šä¼ åŸå›¾'åŠ è½½å›¾ç‰‡ï¼Œç„¶ååœ¨ä¸‹æ–¹ç»˜åˆ¶ Maskã€‚",
        
        "desc_fusion": "**èåˆ2å¼ å›¾ç‰‡**ï¼šå›¾ç‰‡1æä¾›ä¸»è¦ç»“æ„/å§¿åŠ¿ï¼Œå›¾ç‰‡2æä¾›ç»†èŠ‚/è„¸éƒ¨/é£æ ¼ã€‚",
        "label_img1": "å›¾ç‰‡1ï¼ˆä¸»ç»“æ„/å§¿åŠ¿ï¼‰",
        "label_img2": "å›¾ç‰‡2ï¼ˆç»†èŠ‚/è„¸éƒ¨/é£æ ¼ï¼‰",
        "label_fusion_prompt": "èåˆæè¿° Prompt",
        "label_blend": "å›¾ç‰‡2æ··åˆå¼ºåº¦ (0=å…¨ç”¨å›¾ç‰‡1, 1=å…¨ç”¨å›¾ç‰‡2)",
        "label_denoise": "é‡ç»˜å¼ºåº¦ (è¶Šé«˜å˜åŒ–è¶Šå¤§)",
        
        # æ»¤é•œé€‰é¡¹
        "f_blur": "æ¨¡ç³Š", "f_contour": "è½®å»“", "f_detail": "ç»†èŠ‚",
        "f_edge": "è¾¹ç¼˜å¢å¼º", "f_edge_more": "æ›´å¤šè¾¹ç¼˜å¢å¼º",
        "f_emboss": "æµ®é›•", "f_find_edge": "æŸ¥æ‰¾è¾¹ç¼˜",
        "f_sharp": "é”åŒ–", "f_smooth": "å¹³æ»‘", "f_smooth_more": "æ›´å¤šå¹³æ»‘",
        
        "msg_scan_done": "âœ… æ‰«æå®Œæˆï¼æ£€æµ‹åˆ° **{}** ä¸ª LoRA æ–‡ä»¶ã€‚",
        "msg_vram_loading": "æ˜¾å­˜çŠ¶æ€åŠ è½½ä¸­...",
        "msg_interrupt": "ğŸ›‘ æ­£åœ¨å¼ºåˆ¶ä¸­æ–­..."
    },
    "en": {
        "title": "# ğŸ¨ Z-Image-Turbo LowVram Edition", 
        "btn_lang": "Language / è¯­è¨€",
        "tab_t2i": "Text to Image",
        "tab_edit": "Image Edit",
        "tab_i2i": "Image to Image",
        "tab_inpaint": "Inpaint",
        "tab_fusion": "Fusion",
        
        "label_prompt": "Prompt",
        "btn_flush_vram": "ğŸ§¹ Flush VRAM",
        "label_vram_threshold": "Auto Flush Threshold (%)",
        
        "acc_lora": "LoRA Settings",
        "btn_refresh_lora": "ğŸ”„ Scan LoRA Directory",
        "txt_no_lora": "*No LoRA files detected*",
        "label_weight": "Weight",
        
        "acc_model": "Model Settings",
        "btn_refresh_model": "ğŸ”„ Refresh Base Model/VAE",
        "label_transformer": "Transformer",
        "label_vae": "VAE",
        "label_perf": "Performance Mode",
        "val_perf_high": "High End (VRAM>=20GB)",
        "val_perf_low": "Low End (Optimized)",
        "label_width": "Width (x16)",
        "label_height": "Height (x16)",
        "label_steps": "Steps",
        "label_cfg": "CFG",
        "label_batch": "Batch Size",
        "label_seed": "Seed",
        "label_random_seed": "Random Seed",
        "label_sampler": "Sampler",
        
        "btn_run": "ğŸš€ Generate",
        "btn_stop": "ğŸ›‘ Stop Generation",
        "btn_gen": "ğŸš€ Generate",
        "btn_stop_short": "ğŸ›‘ Stop",
        "label_output": "Output Results",
        "label_gallery_i2i": "Img2Img Results",
        "label_gallery_inpaint": "Inpaint Results",
        "label_gallery_fusion": "Fusion Results",
        
        "label_upload_img": "Upload Image",
        "label_rotate": "Rotation (Deg)",
        "label_crop_x": "Crop X (%)",
        "label_crop_y": "Crop Y (%)",
        "label_crop_w": "Crop Width (%)",
        "label_crop_h": "Crop Height (%)",
        "label_flip_h": "Flip Horizontal",
        "label_flip_v": "Flip Vertical",
        "btn_edit": "Start Editing",
        "label_edited": "Edited Image",
        "label_filter": "Apply Filter",
        "label_brightness": "Brightness (%)",
        "label_contrast": "Contrast (%)",
        "label_saturation": "Saturation (%)",
        
        "label_ref_img": "Upload Reference",
        "label_prompt_rec": "Prompt (Recommended)",
        "ph_prompt_i2i": "Describe the image you want to generate...",
        "label_out_w": "Output Width (0=Auto)",
        "label_out_h": "Output Height (0=Auto)",
        "tip_res": "**Tip:** If both are 0, aspect ratio is kept automatically near 1024px.",
        "label_strength": "Denoising Strength",
        "label_cfg_fixed": "CFG (Fixed at 0.0 for Turbo)",
        "label_cfg_turbo": "CFG (Turbo Img2Img)",
        "label_cfg_inpaint": "CFG (Inpaint)",
        
        # --- Inpaint ç‰¹æœ‰ ---
        "lbl_inpaint_editor": "Draw Mask (White=Modify, Black=Keep)",
        "lbl_inpaint_tip": "Tip: Upload base image first, then draw Mask below.",

        "desc_fusion": "**æ™ºèƒ½èåˆå›¾**: èåˆä¸¤å¼ å‚è€ƒå›¾çš„ç‰¹å¾ï¼Œç”ŸæˆåŒ…å«ä¸¤å¼ å›¾å…ƒç´ çš„æ–°åœºæ™¯ã€‚æ”¯æŒäººç‰©+äººç‰©ã€äººç‰©+ç‰©å“/åŠ¨ç‰©ç­‰ç»„åˆã€‚",
        "label_img1": "å‚è€ƒå›¾1ï¼ˆç¬¬ä¸€å¼ å›¾ï¼‰",
        "label_img2": "å‚è€ƒå›¾2ï¼ˆç¬¬äºŒå¼ å›¾ï¼‰",
        "label_fusion_prompt": "åœºæ™¯æè¿°ï¼ˆæè¿°ä¸¤å¼ å›¾å¦‚ä½•èåˆï¼Œä¾‹å¦‚ï¼šå›¾ä¸€çš„äººç‰©å’Œå›¾äºŒçš„äººç‰©ååœ¨å…¬å›­é•¿æ¤…ä¸ŠèŠå¤©ï¼‰",
        "label_blend": "èåˆæƒé‡ï¼ˆ0=åå‘å›¾1ï¼Œ1=åå‘å›¾2ï¼Œ0.5=å¹³è¡¡èåˆï¼‰",
        "label_denoise": "é‡ç»˜å¼ºåº¦ï¼ˆæ•°å€¼è¶Šå¤§ï¼Œå˜åŒ–è¶Šå¤§ï¼Œå»ºè®®0.5-0.8ï¼‰",
        
        # æ»¤é•œé€‰é¡¹
        "f_blur": "Blur", "f_contour": "Contour", "f_detail": "Detail",
        "f_edge": "Edge Enhance", "f_edge_more": "Edge Enhance More",
        "f_emboss": "Emboss", "f_find_edge": "Find Edges",
        "f_sharp": "Sharpen", "f_smooth": "Smooth", "f_smooth_more": "Smooth More",
        
        "msg_scan_done": "âœ… Scan Complete! Detected **{}** LoRA files.",
        "msg_vram_loading": "VRAM Status Loading...",
        "msg_interrupt": "ğŸ›‘ Force Interrupting..."
    }
}

# ==========================================
# é‡‡æ ·å™¨åˆ—è¡¨ä¸é…ç½®
# ==========================================
SAMPLER_LIST = [
    "Default (Z-Image)",   # ã€é»˜è®¤ã€‘ä¸è¿›è¡Œä»»ä½•æ›¿æ¢ï¼Œä¿æŒåŸæœ‰æ­£å¸¸å·¥ä½œçŠ¶æ€
    "DPM++ 2M Karras",     # ã€æ¨èã€‘åŠ¨æ€åç§»ï¼Œé€‚åˆZ-Image
    "DPM++ 2S Karras",     # åŠ¨æ€åç§»
    "Euler a",
    "Euler",
    "DDIM",
    "UniPC"
]

import inspect

class CompatibleScheduler:
    """
    å…¼å®¹æ€§åŒ…è£…å™¨ï¼šè¿‡æ»¤æ‰ Z-Image ä¼ ç»™è°ƒåº¦å™¨çš„å¤šä½™å‚æ•°ï¼ˆå¦‚ 'mu'ï¼‰ï¼Œ
    åŒæ—¶å¼ºåˆ¶åˆ é™¤å¯èƒ½å­˜åœ¨çš„ 'mu' é˜²æ­¢åº•å±‚æŠ¥é”™ã€‚
    """
    def __init__(self, scheduler):
        self._scheduler = scheduler

    def set_timesteps(self, num_inference_steps, device=None, **kwargs):
        # å¼ºåˆ¶ç§»é™¤ 'mu'ï¼Œæ— è®ºè°ƒåº¦å™¨æ˜¯å¦å£°ç§°æ”¯æŒå®ƒ
        kwargs.pop('mu', None)
        
        sig = inspect.signature(self._scheduler.set_timesteps)
        valid_params = set(sig.parameters.keys())
        filtered_kwargs = {k: v for k, v in kwargs.items() if k in valid_params}
        
        return self._scheduler.set_timesteps(num_inference_steps, device=device, **filtered_kwargs)
    
    def step(self, *args, **kwargs):
        return self._scheduler.step(*args, **kwargs)

    def __getattr__(self, name):
        return getattr(self._scheduler, name)

    def __setattr__(self, name, value):
        if name.startswith('_'):
            super().__setattr__(name, value)
        else:
            setattr(self._scheduler, name, value)

def get_scheduler(sampler_name, config):
    """
    è·å–è°ƒåº¦å™¨å®ä¾‹ã€‚
    å¦‚æœæ˜¯ Defaultï¼Œè¿”å› Noneï¼ˆä¸æ›¿æ¢ï¼‰ã€‚
    å¦‚æœæ˜¯ DPM++ï¼Œå¼ºåˆ¶å¼€å¯åŠ¨æ€åç§»ä»¥å…¼å®¹ Z-Imageã€‚
    """
    base_scheduler = None
    
    # ã€ç­–ç•¥ã€‘å¦‚æœæ˜¯é»˜è®¤é€‰é¡¹ï¼Œç›´æ¥è¿”å› Noneï¼Œä¸å¹²é¢„åŸæœ‰çš„è°ƒåº¦å™¨
    # è¿™æ ·å¯ä»¥ä¿è¯å‡ºå›¾æ­£å¸¸ï¼Œé¿å…é›ªèŠ±é—®é¢˜
    if sampler_name == "Default (Z-Image)":
        return None

    try:
        if sampler_name == "DPM++ 2M Karras":
            base_scheduler = DPMSolverMultistepScheduler.from_config(
                config, 
                algorithm="DPM++ 2M", 
                use_karras_sigmas=True,
                use_dynamic_shifting=True,      # å…³é”®ï¼šå¼€å¯åŠ¨æ€åç§»
                time_shift_type="exponential"   # å…³é”®ï¼šæŒ‡æ•°å‹åç§»
            )
        elif sampler_name == "DPM++ 2S Karras":
            base_scheduler = DPMSolverSinglestepScheduler.from_config(
                config, 
                use_karras_sigmas=True,
                use_dynamic_shifting=True,
                time_shift_type="exponential"
            )
        elif sampler_name == "Euler a":
            base_scheduler = EulerAncestralDiscreteScheduler.from_config(config)
        elif sampler_name == "Euler":
            base_scheduler = EulerDiscreteScheduler.from_config(config)
        elif sampler_name == "DDIM":
            base_scheduler = DDIMScheduler.from_config(config)
        elif sampler_name == "UniPC":
            base_scheduler = UniPCMultistepScheduler.from_config(config)
        else:
            # é»˜è®¤å›é€€
            base_scheduler = EulerAncestralDiscreteScheduler.from_config(config)

    except Exception as e:
        print(f"Scheduler init failed for {sampler_name}, fallback to Euler a: {e}")
        base_scheduler = EulerAncestralDiscreteScheduler.from_config(config)

    # è¿”å›åŒ…è£…åçš„è°ƒåº¦å™¨
    return CompatibleScheduler(base_scheduler)

# ==========================================
# åŸºç¡€å‡½æ•°
# ==========================================

def process_mask_for_inpaint(mask_image):
    """å¤„ç†Maskå›¾åƒï¼Œä¸ºinpaintåšå‡†å¤‡"""
    if mask_image is None:
        return None
    if mask_image.mode == 'RGBA':
        import numpy as np
        mask_array = np.array(mask_image)
        alpha = mask_array[:, :, 3] if mask_array.shape[2] > 3 else None
        rgb = mask_array[:, :, :3]
        rgb_gray = np.dot(rgb, [0.299, 0.587, 0.114])
        if alpha is not None:
            mask_gray = np.where(alpha > 10, 255, 0).astype(np.uint8)
        else:
            mask_gray = np.where(rgb_gray > 10, 255, 0).astype(np.uint8)
        mask = Image.fromarray(mask_gray, mode='L')
    else:
        if mask_image.mode != 'L':
            mask_image = mask_image.convert('L')
        mask = mask_image.point(lambda p: 255 if p > 10 else 0)
    if mask.getextrema()[1] == 0:
        return None
    return mask

def materialize_vae(vae):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    with torch.no_grad():
        for param in vae.parameters():
            if param.device.type == "meta":
                real = torch.empty_like(param, device=device)
                param.data = real
    vae.to(device)
    vae.eval()

# ==========================================
# è®¾å¤‡æ¢æµ‹ä¸ç¡¬ä»¶æŠ¥å‘Š
# ==========================================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE = torch.bfloat16 if DEVICE == "cuda" else torch.float32
is_interrupted = False

print("\n" + "="*50)
if DEVICE == "cuda":
    GPU_NAME = torch.cuda.get_device_name(0)
    TOTAL_VRAM = torch.cuda.get_device_properties(0).total_memory
    print(f"è¿è¡Œæ¨¡å¼: [ GPU ]".encode('utf-8', errors='replace').decode())
    print(f"æ ¸å¿ƒå‹å·: {GPU_NAME}".encode('utf-8', errors='replace').decode())
    print(f"æ˜¾å­˜æ€»é‡: {TOTAL_VRAM/1024**3:.2f} GB".encode('utf-8', errors='replace').decode())
else:
    TOTAL_VRAM = 0
    print(f"è¿è¡Œæ¨¡å¼: [ CPU ]".encode('utf-8', errors='replace').decode())
print("="*50 + "\n")

def get_vram_info():
    if DEVICE == "cuda":
        reserved = torch.cuda.memory_reserved(0)
        allocated = torch.cuda.get_device_properties(0).total_memory
        usage_pct = (reserved / TOTAL_VRAM) * 100 if TOTAL_VRAM > 0 else 0
        vram_str = f"æ˜¾å­˜å ç”¨: {usage_pct:.1f}% ({reserved/1024**3:.2f}GB / {TOTAL_VRAM/1024**3:.2f}GB)"
    else:
        usage_pct = 0
        vram_str = "æ˜¾å­˜å ç”¨: CPU æ¨¡å¼"
    mem = psutil.virtual_memory()
    ram_str = f"å†…å­˜å ç”¨: {mem.percent:.1f}% ({(mem.total - mem.available)/1024**3:.2f}GB / {mem.total/1024**3:.2f}GB)"
    status = f"{vram_str} ï½œ {ram_str}"
    return usage_pct, status

def auto_flush_vram(threshold=90):
    usage_pct, _ = get_vram_info()
    if usage_pct > threshold:
        gc.collect()
        torch.cuda.empty_cache()
        return True
    return False

def scan_lora_files():
    if not os.path.exists(LORA_ROOT): return []
    return sorted([f for f in os.listdir(LORA_ROOT) if f.lower().endswith(".safetensors")])

def scan_model_items(base_path):
    if not os.path.exists(base_path): return []
    items = []
    for f in os.listdir(base_path):
        full_path = os.path.join(base_path, f)
        if os.path.isdir(full_path):
            items.append(f)
        elif f.lower().endswith((".safetensors", ".bin", ".pt")):
            items.append(f)
    return sorted(items)

LORA_FILES = scan_lora_files()
print(f"å·²æ£€æµ‹åˆ° {len(LORA_FILES)} ä¸ª LoRA æ–‡ä»¶ã€‚".encode('utf-8', errors='replace').decode())

# ==========================================
# æ¨¡å‹ç®¡ç†å™¨
# ==========================================
class ModelManager:
    def __init__(self):
        self.pipe = None 
        self.current_state = {
            "mode": None,      
            "t_choice": None,  
            "v_choice": None,
            "perf_mode": None
        }
        self.current_loras = []
        self.current_weights_map = {} 

    def _clear_pipeline(self):
        if self.pipe is not None:
            print(f"æ­£åœ¨é”€æ¯æ—§ç®¡é“ä»¥é‡Šæ”¾æ˜¾å­˜...".encode('utf-8', errors='replace').decode())
            try:
                self.pipe.unload_lora_weights()
            except:
                pass
            del self.pipe
            self.pipe = None
        if hasattr(sys, 'last_traceback'):
            del sys.last_traceback
        for _ in range(3):
            gc.collect()
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()

    def _init_pipeline_base(self, mode, low_vram=False):
        # ã€ä¿®å¤ã€‘ä¸å†ä¼ é€’ device å‚æ•°ï¼Œé¿å…è¢«å¿½ç•¥ï¼Œæ”¹ç”¨æ˜¾å¼ .to()
        
        # ã€å…³é”®ä¿®å¤ã€‘åœ¨åŠ è½½æ¨¡å‹å‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé˜²æ­¢ä½æ˜¾å­˜æ¨¡å¼ä¸‹å´©æºƒ
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"
        
        if mode == 'txt':
            print(f"åˆå§‹åŒ–åŸºç¡€ Pipeline (æ–‡æˆå›¾)... ç­–ç•¥: {'ä½æ˜¾å­˜ä¼˜åŒ– (Sequential Offload)' if low_vram else 'å…¨æ˜¾å­˜'}".encode('utf-8', errors='replace').decode())
            pipe = ZImagePipeline.from_pretrained(
                DEFAULT_MODEL_PATH, 
                local_files_only=True
            )
        else:
            print(f"åˆå§‹åŒ–åŸºç¡€ Pipeline (å›¾ç”Ÿå›¾/é‡ç»˜)... ç­–ç•¥: {'ä½æ˜¾å­˜ä¼˜åŒ– (Sequential Offload)' if low_vram else 'å…¨æ˜¾å­˜'}".encode('utf-8', errors='replace').decode())
            pipe = ZImageImg2ImgPipeline.from_pretrained(
                DEFAULT_MODEL_PATH, 
                local_files_only=True
            )
        
        # ã€å…³é”®ä¿®å¤ã€‘å…ˆè®¾ç½®deviceä¸ºCPUï¼Œç„¶åç»Ÿä¸€dtypeï¼Œæœ€åenable_sequential_cpu_offload
        if low_vram:
            print(f"  [System] æ­£åœ¨ç»Ÿä¸€æ¨¡å‹ç²¾åº¦ä¸º: {DTYPE} å¹¶å‡†å¤‡ä½æ˜¾å­˜æ¨¡å¼".encode('utf-8', errors='replace').decode())
            pipe.to(device="cpu", dtype=DTYPE)
            pipe.enable_sequential_cpu_offload()
            print(f"  [System] æ¨¡å‹å·²åŠ è½½è‡³ RAM å¹¶å¯ç”¨ Sequential Offloadã€‚".encode('utf-8', errors='replace').decode())
        else:
            print(f"  [System] æ­£åœ¨ç»Ÿä¸€æ¨¡å‹ç²¾åº¦ä¸º: {DTYPE}".encode('utf-8', errors='replace').decode())
            pipe.to(device=DEVICE, dtype=DTYPE)
        
        return pipe

    def _inject_components(self, pipe, t_choice, v_choice, low_vram=False):
        if t_choice != "default":
            t_path = os.path.join(MOD_TRANS_DIR, t_choice)
            if os.path.isfile(t_path):
                print(f"è½½å…¥ Transformer: {t_choice}".encode('utf-8', errors='replace').decode())
                state_dict = load_file(t_path, device="cpu")
                processed = {}
                prefix = "model.diffusion_model."
                for k, v in state_dict.items():
                    new_k = k[len(prefix):] if k.startswith(prefix) else k
                    processed[new_k] = v.to(DTYPE)
                pipe.transformer.load_state_dict(processed, strict=False, assign=True)
                del state_dict, processed
                gc.collect()

        if v_choice != "default":
            vae_path = os.path.join(MOD_VAE_DIR, v_choice)
            print(f"è½½å…¥ VAE: {v_choice}".encode('utf-8', errors='replace').decode())
            
            if low_vram:
                vae_device_map = {"": "cpu"}
            else:
                vae_device_map = None

            if os.path.isfile(vae_path):
                with tempfile.TemporaryDirectory() as tmpdir:
                    config_file_path = os.path.join(tmpdir, "config.json")
                    with open(config_file_path, "w", encoding="utf-8") as f:
                        json.dump(dict(pipe.vae.config), f, indent=2)
                    # ä¿®å¤ dtype å…¼å®¹æ€§
                    try:
                        pipe.vae = AutoencoderKL.from_single_file(vae_path, dtype=DTYPE, config=tmpdir, device_map=vae_device_map)
                    except TypeError:
                        pipe.vae = AutoencoderKL.from_single_file(vae_path, torch_dtype=DTYPE, config=tmpdir, device_map=vae_device_map)
            else:
                try:
                    pipe.vae = AutoencoderKL.from_pretrained(vae_path, dtype=DTYPE, device_map=vae_device_map)
                except TypeError:
                    pipe.vae = AutoencoderKL.from_pretrained(vae_path, torch_dtype=DTYPE, device_map=vae_device_map)
        return pipe

    def _apply_loras(self, pipe, selected_loras, weights_map):
        if self.current_loras == selected_loras and self.current_weights_map == weights_map:
            return

        print("æ­£åœ¨é…ç½® LoRA...".encode('utf-8', errors='replace').decode())
        try:
            pipe.unload_lora_weights()
        except Exception:
            pass

        if not selected_loras:
            self.current_loras = []
            self.current_weights_map = {}
            return

        active_adapters = []
        adapter_weights = []

        for lora_file in selected_loras:
            adapter_name = re.sub(r"[^a-zA-Z0-9_]", "_", os.path.splitext(lora_file)[0])
            weight = weights_map.get(lora_file, 1.0)
            try:
                pipe.load_lora_weights(LORA_ROOT, weight_name=lora_file, adapter_name=adapter_name)
                active_adapters.append(adapter_name)
                adapter_weights.append(weight)
            except Exception as e:
                print(f"LoRA {lora_file} åŠ è½½å¤±è´¥: {e}".encode('utf-8', errors='replace').decode())

        if active_adapters:
            pipe.set_adapters(active_adapters, adapter_weights=adapter_weights)
        
        self.current_loras = list(selected_loras)
        self.current_weights_map = dict(weights_map)

    def get_pipeline(self, t_choice, v_choice, selected_loras, weights_map, mode='txt', perf_mode="é«˜ç«¯æœº (æ˜¾å­˜>=20GB)"):
        is_low_vram = (
            "ä½ç«¯æœº" in perf_mode or 
            "æ˜¾å­˜ä¼˜åŒ–" in perf_mode or 
            "Low End" in perf_mode or 
            "Optimized" in perf_mode
        )

        need_rebuild = (
            self.pipe is None or
            self.current_state["mode"] != mode or
            self.current_state["t_choice"] != t_choice or
            self.current_state["v_choice"] != v_choice or
            self.current_state["perf_mode"] != perf_mode
        )

        if need_rebuild:
            self._clear_pipeline() 
            try:
                # 1. åˆå§‹åŒ–å¹¶å¼ºåˆ¶ç§»å…¥ CPU (å¦‚æœ low_vram)
                temp_pipe = self._init_pipeline_base(mode, low_vram=is_low_vram)
                temp_pipe = self._inject_components(temp_pipe, t_choice, v_choice, low_vram=is_low_vram)
                
                if DEVICE == "cuda":
                    if is_low_vram:
                        # 2. å¼€å¯ Sequential Offload (ç”¨æˆ·æŒ‡å®š)
                        print("  [System] å·²å¯ç”¨ä½æ˜¾å­˜ä¼˜åŒ–æ¨¡å¼".encode('utf-8', errors='replace').decode())
                        temp_pipe.enable_sequential_cpu_offload()
                    else:
                        print("  [System] å·²å¯ç”¨é«˜ç«¯æœºæ¨¡å¼ (Full CUDA)".encode('utf-8', errors='replace').decode())
                        temp_pipe.to("cuda")

                self.pipe = temp_pipe
                self.current_state = {
                    "mode": mode, "t_choice": t_choice, "v_choice": v_choice, "perf_mode": perf_mode
                }
                self.current_loras = [] 
                self.current_weights_map = {}
                
            except Exception as e:
                self._clear_pipeline()
                raise gr.Error(f"æ¨¡å‹åŠ è½½å´©æºƒ: {str(e)}")

        self._apply_loras(self.pipe, selected_loras, weights_map)
        return self.pipe

manager = ModelManager()

def make_progress_callback(progress, total_steps, refresh_interval=2):
    def _callback(pipe, step, timestep, callback_kwargs):
        global is_interrupted
        if is_interrupted: raise gr.Error("ä»»åŠ¡å·²æ‰‹åŠ¨åœæ­¢")
        step_idx = step + 1
        frac = step_idx / total_steps
        status_suffix = ""
        if step_idx % refresh_interval == 0 or step_idx == total_steps:
            _, mem_status = get_vram_info()
            status_suffix = f"\n{mem_status}"
        progress(frac, desc=f"Diffusion Step {step_idx}/{total_steps}{status_suffix}")
        return callback_kwargs
    return _callback

# ==========================================
# æ ¸å¿ƒé€»è¾‘
# ==========================================
def process_lora_inputs(lora_checks, lora_weights):
    selected = []
    weights_map = {}
    for i, fname in enumerate(LORA_FILES):
        if i < len(lora_checks) and lora_checks[i]:
            selected.append(fname)
            if i < len(lora_weights):
                weights_map[fname] = lora_weights[i]
            else:
                weights_map[fname] = 1.0
    return selected, weights_map

def refresh_lora_list(lang="zh"):
    global LORA_FILES
    LORA_FILES = scan_lora_files()
    count = len(LORA_FILES)
    t = I18N.get(lang, I18N["zh"])
    msg = t["msg_scan_done"].format(count)
    return gr.update(value=msg)

def update_prompt_ui_base(prompt, *lora_ui_args):
    num_loras = len(LORA_FILES)
    if num_loras == 0: return prompt
    checks = lora_ui_args[:num_loras]
    weights = lora_ui_args[num_loras:num_loras*2]
    clean_p = re.sub(r"\s*<lora:[^>]+>", "", prompt or "").strip()
    new_tags = []
    for i, fname in enumerate(LORA_FILES):
        if i < len(checks) and checks[i]:
            w = weights[i] if i < len(weights) else 1.0
            name = os.path.splitext(fname)[0]
            alpha_str = f"{w:.2f}".rstrip("0").rstrip(".")
            new_tags.append(f"<lora:{name}:{alpha_str}>")
    if new_tags:
        return f"{clean_p} {' '.join(new_tags)}"
    else:
        return clean_p

def run_inference(*args):
    global is_interrupted
    is_interrupted = False
    idx = 0
    prompt = args[idx]; idx += 1
    num_loras = len(LORA_FILES)
    lora_checks = args[idx : idx+num_loras]; idx += num_loras
    lora_weights = args[idx : idx+num_loras]; idx += num_loras
    t_choice = args[idx]; idx += 1
    v_choice = args[idx]; idx += 1
    perf_mode = args[idx]; idx += 1
    w = args[idx]; idx += 1
    h = args[idx]; idx += 1
    steps = args[idx]; idx += 1
    cfg = args[idx]; idx += 1
    seed = args[idx]; idx += 1
    is_random = args[idx]; idx += 1
    batch_size = args[idx]; idx += 1
    vram_threshold = args[idx]; idx += 1
    # --- æ–°å¢ï¼šè¯»å–é‡‡æ ·å™¨å‚æ•° ---
    sampler_name = args[idx]; idx += 1

    auto_flush_vram(vram_threshold)
    clean_w = (int(w) // 16) * 16
    clean_h = (int(h) // 16) * 16
    selected_loras, weights_map = process_lora_inputs(lora_checks, lora_weights)
    
    new_tags = []
    if selected_loras:
        tags = []
        for f in selected_loras:
            w_val = weights_map.get(f, 1.0)
            name = os.path.splitext(f)[0]
            alpha_str = f"{w_val:.2f}".rstrip("0").rstrip(".")
            tags.append(f"<lora:{name}:{alpha_str}>")
        if tags:
            new_tags = tags
    
    if new_tags:
        clean_p = re.sub(r"\s*<lora:[^>]+>", "", prompt or "").strip()
        final_prompt = f"{clean_p} {' '.join(new_tags)}"
    else:
        final_prompt = prompt

    try:
        pipe = manager.get_pipeline(t_choice, v_choice, selected_loras, weights_map, mode='txt', perf_mode=perf_mode)
        # ã€å…³é”®ä¿®å¤ã€‘è·å–pipelineåè¿›è¡Œæ˜¾å­˜æ¸…ç†ï¼Œé˜²æ­¢åŠ è½½æ—¶å´©æºƒ
        gc.collect()
        if DEVICE == "cuda":
            torch.cuda.empty_cache()
    except Exception as e:
        raise gr.Error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")

    # --- æ–°å¢ï¼šåº”ç”¨é‡‡æ ·å™¨ ---
    if sampler_name:
        new_scheduler = get_scheduler(sampler_name, pipe.scheduler.config)
        if new_scheduler is not None:
            print(f"Sampler set to: {sampler_name}")
            pipe.scheduler = new_scheduler
        else:
            print(f"Using Default Scheduler for Z-Image.")

    if is_random: seed = random.randint(0, 2**32 - 1)
    generator = torch.Generator(DEVICE).manual_seed(int(seed))

    date_folder = datetime.now().strftime("%Y-%m-%d")
    save_dir = os.path.join(OUTPUT_ROOT, date_folder)
    os.makedirs(save_dir, exist_ok=True)

    results_images = []
    progress = gr.Progress()

    try:
        print(f"ä»»åŠ¡å¯åŠ¨ | å›¾ç‰‡åˆ†è¾¨ç‡: {clean_w}x{clean_h} | ç§å­: {seed}".encode('utf-8', errors='replace').decode())
        step_callback = make_progress_callback(progress, int(steps))

        for i in range(int(batch_size)):
            if is_interrupted: break

            # ç”Ÿæˆæ¯å¼ å›¾ç‹¬ç«‹ seed
            seed_i = random.randint(0, 2**32 - 1)
            generator_i = torch.Generator(DEVICE).manual_seed(seed_i)

            # Prompt å¾®æ‰°
            prompt_i = augment_prompt(prompt)
            final_prompt_i = final_prompt.replace(prompt, prompt_i) if final_prompt != prompt else prompt_i

            output = pipe(
                prompt=final_prompt_i,
                width=clean_w,
                height=clean_h,
                num_inference_steps=int(steps),
                guidance_scale=float(cfg),
                generator=generator_i,
                callback_on_step_end=step_callback
            ).images[0]

            filename = f"{datetime.now().strftime('%H%M%S')}_{uuid.uuid4().hex[:4]}.png"
            path = os.path.join(save_dir, filename)
            output.save(path)
            results_images.append(output)
            _, current_status = get_vram_info()
            yield results_images, seed_i, current_status

    except Exception as e:
        if "ä»»åŠ¡å·²æ‰‹åŠ¨åœæ­¢" in str(e):
            print("ä»»åŠ¡å·²åœæ­¢".encode('utf-8', errors='replace').decode())
        else:
            import traceback
            traceback.print_exc()
            raise gr.Error(f"ç”Ÿæˆä¸­æ–­: {str(e)}")
    finally:
        auto_flush_vram(vram_threshold)

def run_img2img(*args):
    import torch
    from PIL import Image
    import os
    from datetime import datetime

    global is_interrupted
    is_interrupted = False

    idx = 0
    prompt = args[idx]; idx += 1
    negative_prompt = args[idx]; idx += 1
    input_image_path = args[idx]; idx += 1 
    
    if input_image_path is None or input_image_path == "":
         raise gr.Error("è¯·å…ˆä¸Šä¼ å‚è€ƒå›¾ç‰‡")
         
    if os.path.exists(input_image_path):
        file_size_mb = os.path.getsize(input_image_path) / (1024 * 1024)
        if file_size_mb > 10.0:
            raise gr.Error(f"âŒ å›¾ç‰‡è¿‡å¤§ï¼\nè¯·ä¸Šä¼ å°äº 10.0MB çš„å›¾ç‰‡ã€‚\n(å½“å‰å›¾ç‰‡å¤§å°: {file_size_mb:.2f}MB)")
        input_image = Image.open(input_image_path).convert("RGB")
    else:
        input_image = None

    width_slider = args[idx]; idx += 1
    height_slider = args[idx]; idx += 1
    steps_ui = args[idx]; idx += 1
    cfg_ui = args[idx]; idx += 1
    strength_ui = args[idx]; idx += 1
    seed = args[idx]; idx += 1
    t_choice = args[idx]; idx += 1
    v_choice = args[idx]; idx += 1
    perf_mode = args[idx]; idx += 1
    lora_name = args[idx]; idx += 1
    lora_weight = args[idx]; idx += 1
    img2img_mode = args[idx]; idx += 1
    # --- æ–°å¢ï¼šè¯»å–é‡‡æ ·å™¨å‚æ•° ---
    sampler_name = args[idx]; idx += 1

    if input_image is None:
        raise gr.Error("è¯·å…ˆä¸Šä¼ å‚è€ƒå›¾ç‰‡")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    generator = None if seed < 0 else torch.Generator(device=device).manual_seed(int(seed))

    orig_width, orig_height = input_image.width, input_image.height
    width, height = orig_width, orig_height
    input_image = input_image.convert("RGB")

    if img2img_mode.startswith("A"):
        strength = 0.30; steps = 8; cfg = 1.0; lora_scale = 0.35
    else:
        strength = 0.45; steps = 6; cfg = 1.5; lora_scale = 0.65

    selected_loras = []
    weights_map = {}
    lora_prompt = ""

    if lora_name not in (None, "None", "") and float(lora_weight) > 0:
        selected_loras = [lora_name]
        effective_weight = float(lora_weight) * lora_scale
        weights_map = {lora_name: effective_weight}
        lora_prompt = f"<lora:{lora_name}:{effective_weight:.2f}> "

    final_prompt = f"{lora_prompt}{augment_prompt(prompt)}".strip()

    try:
        pipe = manager.get_pipeline(
            t_choice, v_choice, selected_loras, weights_map,
            mode="img", perf_mode=perf_mode
        )
    except Exception as e:
        raise gr.Error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")

    # --- æ–°å¢ï¼šåº”ç”¨é‡‡æ ·å™¨ ---
    if sampler_name:
        new_scheduler = get_scheduler(sampler_name, pipe.scheduler.config)
        if new_scheduler is not None:
            print(f"Sampler set to: {sampler_name}")
            pipe.scheduler = new_scheduler

    def step_callback(pipe, step_index, timestep, callback_kwargs):
        if is_interrupted: raise gr.Error("ä»»åŠ¡å·²æ‰‹åŠ¨åœæ­¢")
        return callback_kwargs

    try:
        with torch.inference_mode():
            result = pipe(
                prompt=final_prompt, negative_prompt=negative_prompt,
                image=input_image, strength=strength,
                num_inference_steps=steps, guidance_scale=cfg,
                generator=generator, callback_on_step_end=step_callback
            ).images[0]
    except Exception as e:
        raise gr.Error(str(e))

    output_root = "outputs"
    os.makedirs(output_root, exist_ok=True)
    date_str = datetime.now().strftime("%Y-%m-%d")
    output_dir = os.path.join(output_root, date_str)
    os.makedirs(output_dir, exist_ok=True)
    time_str = datetime.now().strftime("%H-%M-%S")
    output_path = os.path.join(output_dir, f"{time_str}.png")
    result.save(output_path, format="PNG")
    print(f"å›¾åƒå·²ä¿å­˜: {output_path}".encode('utf-8', errors='replace').decode())
    pipe.unfuse_lora()

    return [result], seed, get_vram_info()[1]

def run_inpainting(*args):
    import torch
    from PIL import Image, ImageOps
    import os
    from datetime import datetime

    global is_interrupted
    is_interrupted = False

    idx = 0
    # ä»ImageEditorä¸­è·å–å›¾åƒå’ŒMask
    image_editor_data = args[idx]; idx += 1
     
    # éªŒè¯è¾“å…¥
    if image_editor_data is None:
        raise gr.Error("è¯·å…ˆä¸Šä¼ åŸå›¾")
     
    # å¤„ç†ImageEditoræ•°æ® - Gradio ImageEditorè¿”å›ç‰¹å®šæ ¼å¼
    print(f"DEBUG: ImageEditoræ•°æ®ç±»å‹: {type(image_editor_data)}".encode('utf-8', errors='replace').decode())
    if isinstance(image_editor_data, dict):
        print(f"DEBUG: ImageEditoré”®: {list(image_editor_data.keys())}".encode('utf-8', errors='replace').decode())
     
    # å°è¯•å¤šç§å¯èƒ½çš„æ•°æ®æ ¼å¼
    input_image = None
    mask_layer = None
     
    # æ ¼å¼1ï¼šGradio ImageEditoræ ‡å‡†æ ¼å¼ {'background': ..., 'layers': [...], 'composite': ...}
    if isinstance(image_editor_data, dict) and 'background' in image_editor_data:
        input_image = image_editor_data.get('background')
        # ä»layersä¸­æå–maskä¿¡æ¯
        if image_editor_data.get('layers'):
            mask_layer = image_editor_data['layers'][0]  # ç¬¬ä¸€ä¸ªå›¾å±‚æ˜¯mask
        print("DEBUG: ä½¿ç”¨Gradio ImageEditoræ ‡å‡†æ ¼å¼".encode('utf-8', errors='replace').decode())
        
    # æ ¼å¼2ï¼šå…ƒç»„ (image, mask)
    elif isinstance(image_editor_data, (tuple, list)) and len(image_editor_data) == 2:
        input_image = image_editor_data[0]
        mask_layer = image_editor_data[1]
        print("DEBUG: ä½¿ç”¨å…ƒç»„æ ¼å¼ (image, mask)".encode('utf-8', errors='replace').decode())
        
    # æ ¼å¼3ï¼šå­—å…¸ {'image': ..., 'mask': ...}
    elif isinstance(image_editor_data, dict) and 'image' in image_editor_data:
        input_image = image_editor_data.get('image')
        mask_layer = image_editor_data.get('mask')
        print("DEBUG: ä½¿ç”¨å­—å…¸æ ¼å¼".encode('utf-8', errors='replace').decode())
        
    # æ ¼å¼4ï¼šç›´æ¥æ˜¯å›¾åƒå¯¹è±¡ï¼ˆæ—§ç‰ˆæœ¬å…¼å®¹ï¼‰
    elif isinstance(image_editor_data, Image.Image):
        input_image = image_editor_data.convert("RGB")
        mask_layer = None
        print("DEBUG: ä½¿ç”¨ç›´æ¥å›¾åƒæ ¼å¼".encode('utf-8', errors='replace').decode())
        
    # æ ¼å¼5ï¼šæ–‡ä»¶è·¯å¾„
    elif isinstance(image_editor_data, str) and os.path.exists(image_editor_data):
        input_image = Image.open(image_editor_data).convert("RGB")
        mask_layer = None
        print("DEBUG: ä½¿ç”¨æ–‡ä»¶è·¯å¾„æ ¼å¼".encode('utf-8', errors='replace').decode())
     
    # éªŒè¯å›¾åƒæ˜¯å¦åŠ è½½æˆåŠŸ
    if input_image is None:
        raise gr.Error(f"æ— æ³•åŠ è½½å›¾åƒæ•°æ®ã€‚å½“å‰æ•°æ®ç±»å‹: {type(image_editor_data)}")
     
    # å¤„ç†RGBAå›¾åƒ - ImageEditorè¿”å›RGBAæ ¼å¼
    if input_image.mode == 'RGBA':
        # åˆ›å»ºç™½è‰²èƒŒæ™¯
        background = Image.new('RGB', input_image.size, (255, 255, 255))
        # å°†RGBAå›¾åƒç²˜è´´åˆ°ç™½è‰²èƒŒæ™¯ä¸Š
        background.paste(input_image, (0, 0), input_image)
        input_image = background
    else:
        input_image = input_image.convert("RGB")
     
    # å¤„ç†Mask - å¦‚æœæ²¡æœ‰ç»˜åˆ¶Maskï¼Œåˆ›å»ºä¸€ä¸ªå…¨é»‘çš„Maskï¼ˆè¡¨ç¤ºå…¨éƒ¨ä¿æŒåŸæ ·ï¼‰
    if mask_layer is None:
        # åˆ›å»ºä¸€ä¸ªå…¨é»‘çš„Maskï¼Œè¡¨ç¤ºå…¨éƒ¨ä¿æŒåŸæ ·
        mask_layer = Image.new('L', input_image.size, color=0)
    elif isinstance(mask_layer, str):
        if os.path.exists(mask_layer):
            mask_layer = Image.open(mask_layer)
        else:
            raise gr.Error("Maskæ–‡ä»¶ä¸å­˜åœ¨")
     
    # å¤„ç†Mask
    if isinstance(mask_layer, str):
        if os.path.exists(mask_layer):
            mask_layer = Image.open(mask_layer)
        else:
            raise gr.Error("Maskæ–‡ä»¶ä¸å­˜åœ¨")
    elif not isinstance(mask_layer, Image.Image):
        raise gr.Error(f"Maskæ ¼å¼é”™è¯¯: {type(mask_layer)}")
     
    # ä½¿ç”¨ä¸“é—¨çš„Maskå¤„ç†å‡½æ•°
    mask = process_mask_for_inpaint(mask_layer)
    if mask is None:
        raise gr.Error("Mask ä¸ºç©ºæˆ–æ— æ•ˆï¼Œè¯·ä½¿ç”¨ç”»ç¬”åœ¨å›¾ç‰‡ä¸Šæ¶‚æŠ¹è¦ä¿®æ”¹çš„åŒºåŸŸã€‚\n\nä½¿ç”¨è¯´æ˜ï¼š\nâ€¢ æ¶‚æŠ¹åŒºåŸŸï¼ˆä»»ä½•é¢œè‰²ï¼‰= éœ€è¦ä¿®æ”¹çš„éƒ¨åˆ†\nâ€¢ æœªæ¶‚æŠ¹åŒºåŸŸ = ä¿æŒåŸæ ·çš„éƒ¨åˆ†\n\nğŸ’¡ æç¤ºï¼šå¯ä»¥ä½¿ç”¨ä»»æ„é¢œè‰²çš„ç”»ç¬”è¿›è¡Œæ¶‚æŠ¹ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è¯†åˆ«ã€‚")
     
    # ç¡®ä¿å›¾åƒå’ŒMaskå°ºå¯¸åŒ¹é…
    orig_width, orig_height = input_image.size
    if mask.size != (orig_width, orig_height):
        print(f"è°ƒæ•´Maskå°ºå¯¸: {mask.size} -> ({orig_width}, {orig_height})".encode('utf-8', errors='replace').decode())
        mask = mask.resize((orig_width, orig_height), Image.LANCZOS)
     
    # æ£€æŸ¥Maskæ˜¯å¦æœ‰æ¶‚æŠ¹åŒºåŸŸ
    # å¦‚æœMaskå…¨æ˜¯é»‘è‰²ï¼ˆæœ€å¤§å€¼ä¸º0ï¼‰ï¼Œè¡¨ç¤ºç”¨æˆ·æ²¡æœ‰ç»˜åˆ¶ä»»ä½•è¦ä¿®æ”¹çš„åŒºåŸŸ
    # è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå°çš„æ¶‚æŠ¹åŒºåŸŸåœ¨ä¸­é—´ä½œä¸ºç¤ºä¾‹
    if mask.getextrema()[1] == 0:  # æœ€å¤§å€¼ä¸º0ï¼Œè¯´æ˜å…¨æ˜¯é»‘è‰²ï¼Œæ²¡æœ‰æ¶‚æŠ¹
        # åˆ›å»ºä¸€ä¸ªç¤ºä¾‹Maskï¼Œåœ¨ä¸­é—´æœ‰ä¸€ä¸ªå°çš„æ¶‚æŠ¹åŒºåŸŸ
        mask = Image.new('L', (orig_width, orig_height), color=0)
        draw = ImageDraw.Draw(mask)
        # åœ¨ä¸­é—´ç»˜åˆ¶ä¸€ä¸ªå°çš„ç™½è‰²çŸ©å½¢ä½œä¸ºç¤ºä¾‹
        box_size = min(orig_width, orig_height) // 4
        left = (orig_width - box_size) // 2
        top = (orig_height - box_size) // 2
        draw.rectangle([left, top, left + box_size, top + box_size], fill=255)
        print("ç”¨æˆ·æ²¡æœ‰ç»˜åˆ¶ä»»ä½•è¦ä¿®æ”¹çš„åŒºåŸŸï¼Œå·²è‡ªåŠ¨åˆ›å»ºä¸€ä¸ªç¤ºä¾‹Maskåœ¨å›¾ç‰‡ä¸­é—´".encode('utf-8', errors='replace').decode())

    prompt = args[idx]; idx += 1
    negative_prompt = args[idx]; idx += 1
    
    steps_ui = args[idx]; idx += 1
    cfg_ui = args[idx]; idx += 1
    strength_ui = args[idx]; idx += 1
    seed = args[idx]; idx += 1

    t_choice = args[idx]; idx += 1
    v_choice = args[idx]; idx += 1
    perf_mode = args[idx]; idx += 1

    lora_name = args[idx]; idx += 1
    lora_weight = args[idx]; idx += 1
    # --- æ–°å¢ï¼šè¯»å–é‡‡æ ·å™¨å‚æ•° ---
    sampler_name = args[idx]; idx += 1
    
    input_image = input_image.convert("RGB")
    
    selected_loras = []
    weights_map = {}
    lora_prompt = ""
    lora_scale = 0.6 

    if lora_name not in (None, "None", "") and float(lora_weight) > 0:
        selected_loras = [lora_name]
        effective_weight = float(lora_weight) * lora_scale
        weights_map = {lora_name: effective_weight}
        lora_prompt = f"<lora:{lora_name}:{effective_weight:.2f}> "

    final_prompt = f"{lora_prompt}{augment_prompt(prompt)}".strip()

    try:
        pipe = manager.get_pipeline(
            t_choice, v_choice, selected_loras, weights_map,
            mode="img", perf_mode=perf_mode
        )
    except Exception as e:
        raise gr.Error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")

    # --- æ–°å¢ï¼šåº”ç”¨é‡‡æ ·å™¨ ---
    if sampler_name:
        new_scheduler = get_scheduler(sampler_name, pipe.scheduler.config)
        if new_scheduler is not None:
            print(f"Sampler set to: {sampler_name}")
            pipe.scheduler = new_scheduler

    device = "cuda" if torch.cuda.is_available() else "cpu"
    generator = None if seed < 0 else torch.Generator(device=device).manual_seed(int(seed))

    def step_callback(pipe, step_index, timestep, callback_kwargs):
        if is_interrupted: raise gr.Error("ä»»åŠ¡å·²æ‰‹åŠ¨åœæ­¢")
        return callback_kwargs

    try:
        with torch.inference_mode():
            # å°è¯•ä½¿ç”¨æ ‡å‡†çš„inpaintè°ƒç”¨
            # å…ˆå°è¯• mask_image å‚æ•°
            result = None
            try:
                result = pipe(
                    prompt=final_prompt,
                    negative_prompt=negative_prompt,
                    image=input_image,
                    mask_image=mask,
                    strength=float(strength_ui),
                    num_inference_steps=int(steps_ui),
                    guidance_scale=float(cfg_ui),
                    generator=generator,
                    callback_on_step_end=step_callback
                ).images[0]
            except TypeError as te1:
                # å¦‚æœä¸æ”¯æŒmask_imageï¼Œå°è¯• mask å‚æ•°
                try:
                    result = pipe(
                        prompt=final_prompt,
                        negative_prompt=negative_prompt,
                        image=input_image,
                        mask=mask,
                        strength=float(strength_ui),
                        num_inference_steps=int(steps_ui),
                        guidance_scale=float(cfg_ui),
                        generator=generator,
                        callback_on_step_end=step_callback
                    ).images[0]
                except TypeError as te2:
                    # å¦‚æœéƒ½ä¸æ”¯æŒï¼Œä½¿ç”¨æ‰‹åŠ¨inpaintå®ç°
                    print(f"æ ‡å‡†inpaintå¤±è´¥ï¼Œä½¿ç”¨æ‰‹åŠ¨inpaintå®ç°: ...è¯·ç¨ç­‰ç‰‡åˆ»ï¼Œæ­£åœ¨ç”Ÿæˆä¸­...".encode('utf-8', errors='replace').decode())
                    # æ‰‹åŠ¨å®ç°inpaintï¼šå…ˆä½¿ç”¨img2imgç”Ÿæˆï¼Œç„¶åæ ¹æ®maskæ··åˆ
                    import numpy as np
                    
                    # ç¡®ä¿å›¾åƒå’Œmaskå°ºå¯¸åŒ¹é…
                    if mask.size != input_image.size:
                        mask = mask.resize(input_image.size, Image.LANCZOS)
                    
                    # è½¬æ¢ä¸ºnumpyæ•°ç»„
                    img_array = np.array(input_image).astype(np.float32) / 255.0
                    mask_2d = np.array(mask.convert('L')).astype(np.float32) / 255.0
                    
                    # æ‰©å±•maskåˆ°3é€šé“ç”¨äºå›¾åƒæ··åˆ
                    mask_3d = np.expand_dims(mask_2d, axis=2)
                    mask_3d = np.repeat(mask_3d, 3, axis=2)
                    
                    # åœ¨maskåŒºåŸŸæ·»åŠ ä¸€äº›å™ªå£°ï¼Œå¸®åŠ©æ¨¡å‹ç†è§£éœ€è¦é‡ç»˜çš„åŒºåŸŸ
                    noise = np.random.randn(*img_array.shape).astype(np.float32) * 0.1
                    # å°†maskåŒºåŸŸæ›¿æ¢ä¸ºå¸¦å™ªå£°çš„å›¾åƒ
                    inpaint_image = img_array * (1 - mask_3d) + (img_array + noise) * mask_3d
                    inpaint_image = np.clip(inpaint_image, 0, 1)
                    inpaint_image_pil = Image.fromarray((inpaint_image * 255).astype(np.uint8))
                    
                    # ä½¿ç”¨img2imgç”Ÿæˆ
                    generated = pipe(
                        prompt=final_prompt,
                        negative_prompt=negative_prompt,
                        image=inpaint_image_pil,
                        strength=float(strength_ui),
                        num_inference_steps=int(steps_ui),
                        guidance_scale=float(cfg_ui),
                        generator=generator,
                        callback_on_step_end=step_callback
                    ).images[0]
                    
                    # ç¡®ä¿ç”Ÿæˆå›¾åƒçš„å°ºå¯¸ä¸åŸå›¾åŒ¹é…ï¼ˆpipelineå¯èƒ½ä¼šè°ƒæ•´å°ºå¯¸ï¼‰
                    orig_size = input_image.size
                    if generated.size != orig_size:
                        print(f"ç”Ÿæˆå›¾åƒå°ºå¯¸ä¸åŒ¹é…ï¼Œè°ƒæ•´ä¸­: {generated.size} -> {orig_size}".encode('utf-8', errors='replace').decode())
                        generated = generated.resize(orig_size, Image.LANCZOS)
                    
                    # ç¡®ä¿maskå°ºå¯¸ä¹ŸåŒ¹é…ï¼Œå¹¶é‡æ–°åˆ›å»ºmask_3d
                    if mask.size != orig_size:
                        mask = mask.resize(orig_size, Image.LANCZOS)
                    
                    # é‡æ–°åˆ›å»ºmaskæ•°ç»„ï¼Œç¡®ä¿å°ºå¯¸åŒ¹é…
                    mask_2d = np.array(mask.convert('L')).astype(np.float32) / 255.0
                    mask_3d = np.expand_dims(mask_2d, axis=2)
                    mask_3d = np.repeat(mask_3d, 3, axis=2)
                    
                    # å°†ç”Ÿæˆç»“æœä¸åŸå›¾æŒ‰ç…§maskæ··åˆ
                    gen_array = np.array(generated).astype(np.float32) / 255.0
                    orig_array = np.array(input_image).astype(np.float32) / 255.0
                    
                    # éªŒè¯æ‰€æœ‰æ•°ç»„å°ºå¯¸æ˜¯å¦åŒ¹é…
                    if gen_array.shape != orig_array.shape:
                        print(f"å°ºå¯¸ä¸åŒ¹é…: ç”Ÿæˆå›¾åƒ {gen_array.shape} vs åŸå›¾ {orig_array.shape}".encode('utf-8', errors='replace').decode())
                        # å¼ºåˆ¶è°ƒæ•´ç”Ÿæˆå›¾åƒå°ºå¯¸
                        gen_pil = Image.fromarray((gen_array * 255).astype(np.uint8))
                        gen_pil = gen_pil.resize(orig_size, Image.LANCZOS)
                        gen_array = np.array(gen_pil).astype(np.float32) / 255.0
                    
                    if mask_3d.shape[:2] != orig_array.shape[:2]:
                        print(f"Maskå°ºå¯¸ä¸åŒ¹é…: {mask_3d.shape[:2]} vs åŸå›¾ {orig_array.shape[:2]}".encode('utf-8', errors='replace').decode())
                        # é‡æ–°è°ƒæ•´mask
                        mask = mask.resize(orig_size, Image.LANCZOS)
                        mask_2d = np.array(mask.convert('L')).astype(np.float32) / 255.0
                        mask_3d = np.expand_dims(mask_2d, axis=2)
                        mask_3d = np.repeat(mask_3d, 3, axis=2)
                    
                    # æœ€ç»ˆéªŒè¯
                    if gen_array.shape != orig_array.shape or mask_3d.shape != orig_array.shape:
                        raise ValueError(f"å°ºå¯¸éªŒè¯å¤±è´¥: gen={gen_array.shape}, orig={orig_array.shape}, mask={mask_3d.shape}")
                    
                    # æ··åˆï¼šmaskåŒºåŸŸä½¿ç”¨ç”Ÿæˆç»“æœï¼ŒémaskåŒºåŸŸä¿æŒåŸå›¾
                    result_array = orig_array * (1 - mask_3d) + gen_array * mask_3d
                    result_array = np.clip(result_array, 0, 1)
                    result = Image.fromarray((result_array * 255).astype(np.uint8))
    except Exception as e:
        import traceback
        traceback.print_exc()
        if "ä»»åŠ¡å·²æ‰‹åŠ¨åœæ­¢" in str(e):
            raise
        elif "mask_image" in str(e) or "unexpected keyword argument" in str(e):
            raise gr.Error("Pipeline é”™è¯¯: å½“å‰æ¨¡å‹å¯èƒ½ä¸æ”¯æŒ Inpaint (mask_image) åŠŸèƒ½ã€‚\nè¯·æ£€æŸ¥ ZImageImg2ImgPipeline æ˜¯å¦æ”¯æŒ mask_image å‚æ•°ã€‚")
        else:
            raise gr.Error(f"å±€éƒ¨é‡ç»˜å¤±è´¥: {str(e)}")

    output_root = "outputs"
    os.makedirs(output_root, exist_ok=True)
    date_str = datetime.now().strftime("%Y-%m-%d")
    output_dir = os.path.join(output_root, date_str)
    os.makedirs(output_dir, exist_ok=True)
    time_str = datetime.now().strftime("%H-%M-%S")
    output_path = os.path.join(output_dir, f"inpaint_{time_str}.png")
    result.save(output_path, format="PNG")
    print(f"Inpaint å›¾åƒå·²ä¿å­˜: {output_path}".encode('utf-8', errors='replace').decode())

    return [result], seed, get_vram_info()[1]

def run_fusion_img(*args, progress=gr.Progress()):
    """
    æ™ºèƒ½èåˆå›¾åŠŸèƒ½ - å…¨æ–°ä¼˜åŒ–ç‰ˆï¼š
    1. æ™ºèƒ½åœºæ™¯è¯†åˆ«ï¼šè‡ªåŠ¨è¯†åˆ«å­£èŠ‚ã€åœºæ™¯ç±»å‹
    2. äººç‰©å­£èŠ‚åŒ–å¤„ç†ï¼šæ ¹æ®å­£èŠ‚è‡ªåŠ¨è°ƒæ•´äººç‰©ç©¿ç€æ‰“æ‰®
    3. å¤šä¸»ä½“ç±»å‹æ”¯æŒï¼šäººç‰©+äººç‰©ã€ç‰©å“+ç‰©å“ã€äººç‰©+ç‰©å“/åŠ¨ç‰©
    4. ç²¾ç¡®æç¤ºè¯è§£æï¼šè¯†åˆ«å…·ä½“çš„åœºæ™¯ã€åŠ¨ä½œã€äº’åŠ¨è¦æ±‚
    """
    global is_interrupted
    is_interrupted = False
    
    idx = 0
    image1_path = args[idx]; idx += 1
    image2_path = args[idx]; idx += 1
    
    if image1_path is None or image2_path is None:
        raise gr.Error("è¯·ä¸Šä¼ ä¸¤å¼ å‚è€ƒå›¾ç‰‡")
    
    if os.path.exists(image1_path):
        s1 = os.path.getsize(image1_path) / (1024 * 1024)
        if s1 > 10.0:
            raise gr.Error(f"âŒ å›¾ç‰‡1è¿‡å¤§ï¼\nè¯·ä¸Šä¼ å°äº 10.0MB çš„å›¾ç‰‡ã€‚\n(å½“å‰å›¾ç‰‡å¤§å°: {s1:.2f}MB)")
        image1 = Image.open(image1_path).convert("RGB")
    else:
        image1 = None
        
    if os.path.exists(image2_path):
        s2 = os.path.getsize(image2_path) / (1024 * 1024)
        if s2 > 10.0:
            raise gr.Error(f"âŒ å›¾ç‰‡2è¿‡å¤§ï¼\nè¯·ä¸Šä¼ å°äº 10.0MB çš„å›¾ç‰‡ã€‚\n(å½“å‰å›¾ç‰‡å¤§å°: {s2:.2f}MB)")
        image2 = Image.open(image2_path).convert("RGB")
    else:
        image2 = None
    
    prompt = args[idx]; idx += 1
    
    num_loras = len(LORA_FILES)
    lora_checks = args[idx : idx+num_loras]; idx += num_loras
    lora_weights = args[idx : idx+num_loras]; idx += num_loras
    
    t_choice = args[idx]; idx += 1
    v_choice = args[idx]; idx += 1
    perf_mode = args[idx]; idx += 1
    
    output_width = args[idx]; idx += 1
    output_height = args[idx]; idx += 1
    blend_strength = args[idx]; idx += 1  # å›¾1å’Œå›¾2çš„èåˆæƒé‡
    strength = args[idx]; idx += 1  # img2imgå¼ºåº¦
    steps = args[idx]; idx += 1
    cfg = args[idx]; idx += 1
    seed = args[idx]; idx += 1
    is_random = args[idx]; idx += 1
    batch_size = args[idx]; idx += 1
    vram_threshold = args[idx]; idx += 1
    # --- æ–°å¢ï¼šè¯»å–é‡‡æ ·å™¨å‚æ•° ---
    sampler_name = args[idx]; idx += 1

    auto_flush_vram(vram_threshold)
    selected_loras, weights_map = process_lora_inputs(lora_checks, lora_weights)

    if selected_loras:
        tags = []
        for f in selected_loras:
            w_val = weights_map.get(f, 1.0)
            name = os.path.splitext(f)[0]
            tags.append(f"<lora:{name}:{w_val:.2f}>")
        clean_p = re.sub(r"\s*<lora:[^>]+>", "", prompt or "").strip()
        final_prompt = f"{clean_p} {' '.join(tags)}"
    else:
        final_prompt = prompt
    
    # ============ æ–°å¢ï¼šæ™ºèƒ½åœºæ™¯åˆ†æå¼•æ“ ============
    def analyze_prompt_intent(prompt_text):
        """
        æ™ºèƒ½åˆ†ææç¤ºè¯æ„å›¾ï¼Œè¿”å›åœºæ™¯ç±»å‹å’Œç‰¹å¾
        """
        if not prompt_text:
            return {"scene_type": "general", "season": "neutral", "interaction": "standing", "objects": []}
        
        prompt_lower = prompt_text.lower()
        
        # å­£èŠ‚è¯†åˆ«
        season_keywords = {
            "spring": ["æ˜¥å¤©", "æ˜¥å­£", "æ¨±èŠ±", "æ˜¥èŠ±", "ç»¿å¶", "æ˜¥é›¨", "spring", "cherry blossom"],
            "summer": ["å¤å¤©", "å¤å­£", "é˜³å…‰", "æµ·æ»©", "æ¸¸æ³³", "çŸ­è£¤", "è£™å­", "summer", "beach", "sunny"],
            "autumn": ["ç§‹å¤©", "ç§‹å­£", "è½å¶", "é‡‘é»„", "æ¢§æ¡", "æ¯›è¡£", "å¤–å¥—", "å›´å·¾", "autumn", "fall", "orange leaves"],
            "winter": ["å†¬å¤©", "å†¬å­£", "é›ª", "é›ªèŠ±", "ç¾½ç»’æœ", "æ‰‹å¥—", "winter", "snow", "snowflakes"]
        }
        
        detected_season = "neutral"
        for season, keywords in season_keywords.items():
            if any(kw in prompt_lower for kw in keywords):
                detected_season = season
                break
        
        # åœºæ™¯è¯†åˆ«
        scene_keywords = {
            "park": ["å…¬å›­", "é•¿æ¤…", "bench", "park", "tree"],
            "garden": ["èŠ±å›­", "èŠ±å›", "garden", "flowers"],
            "street": ["è¡—é“", "è¡—", "street", "road"],
            "indoor": ["å®¤å†…", "æˆ¿é—´", "indoor", "room", "home"],
            "nature": ["è‡ªç„¶", "é‡å¤–", "æ£®æ—", "nature", "forest", "outdoor"]
        }
        
        detected_scene = "general"
        for scene, keywords in scene_keywords.items():
            if any(kw in prompt_lower for kw in keywords):
                detected_scene = scene
                break
        
        # äº’åŠ¨ç±»å‹è¯†åˆ«
        interaction_keywords = {
            "sitting": ["å", "åç€", "ååœ¨ä¸€èµ·", "sitting", "sit"],
            "standing": ["ç«™", "ç«™ç€", "standing", "stand"],
            "walking": ["èµ°", "èµ°è·¯", "walking", "walk"],
            "talking": ["èŠå¤©", "äº¤è°ˆ", "è¯´è¯", "talking", "chat", "conversation"],
            "playing": ["ç©", "æ¸¸æˆ", "playing", "play"]
        }
        
        detected_interaction = "standing"
        for interaction, keywords in interaction_keywords.items():
            if any(kw in prompt_lower for kw in keywords):
                detected_interaction = interaction
                break
        
        # ç‰©ä½“è¯†åˆ«
        detected_objects = []
        object_keywords = {
            "bench": ["é•¿æ¤…", "bench"],
            "table": ["æ¡Œå­", "table"],
            "cat": ["çŒ«", "çŒ«å’ª", "cat", "kitten"],
            "dog": ["ç‹—", "ç‹—ç‹—", "dog", "puppy"],
            "car": ["è½¦", "æ±½è½¦", "car", "vehicle"],
            "tree": ["æ ‘", "æ ‘æœ¨", "tree", "trees"]
        }
        
        for obj, keywords in object_keywords.items():
            if any(kw in prompt_lower for kw in keywords):
                detected_objects.append(obj)
        
        return {
            "scene_type": detected_scene,
            "season": detected_season,
            "interaction": detected_interaction,
            "objects": detected_objects
        }
    
    def generate_seasonal_clothing_prompt(season, base_character_desc=""):
        """
        æ ¹æ®å­£èŠ‚ç”Ÿæˆç›¸åº”çš„ç©¿ç€æ‰“æ‰®æè¿°
        """
        clothing_prompts = {
            "autumn": {
                "general": "wearing warm autumn clothing, sweater, coat, scarf, layered clothes, cozy autumn outfit",
                "detailed": "wearing autumn outfit: sweater, cardigan, long sleeves, scarf, warm pants, boots, layered clothing for mild weather"
            },
            "winter": {
                "general": "wearing winter clothing, coat, gloves, scarf, warm layers, winter jacket",
                "detailed": "wearing winter outfit: thick coat, winter jacket, gloves, warm scarf, winter boots, thermal clothing"
            },
            "spring": {
                "general": "wearing spring clothing, light jacket, casual outfit, spring style",
                "detailed": "wearing spring outfit: light sweater, denim jacket, casual pants, spring colors, fresh and light clothing"
            },
            "summer": {
                "general": "wearing summer clothing, light shirt, shorts, summer outfit",
                "detailed": "wearing summer outfit: t-shirt, shorts or light pants, summer dress, breathable fabric, casual summer style"
            }
        }
        
        if season in clothing_prompts:
            return clothing_prompts[season]["detailed"]
        else:
            return "wearing stylish clothing, well-dressed"
    
    def enhance_prompt_for_scene_analysis(prompt_text, analysis_result):
        """
        åŸºäºåœºæ™¯åˆ†æç»“æœå¢å¼ºæç¤ºè¯
        """
        enhanced_prompt = prompt_text
        
        # å¦‚æœæ£€æµ‹åˆ°å­£èŠ‚ï¼Œæ·»åŠ å­£èŠ‚ç‰¹å¾
        if analysis_result["season"] != "neutral":
            season_clothing = generate_seasonal_clothing_prompt(analysis_result["season"])
            if "äººç‰©" in prompt_text or "person" in prompt_text.lower():
                # é’ˆå¯¹äººç‰©åœºæ™¯ï¼Œæ·»åŠ å­£èŠ‚æ€§ç©¿ç€
                enhanced_prompt += f", {season_clothing}"
            
            # æ·»åŠ å­£èŠ‚æ€§ç¯å¢ƒæè¿°
            if analysis_result["season"] == "autumn":
                enhanced_prompt += ", autumn leaves falling, golden autumn atmosphere, fallen leaves on ground"
            elif analysis_result["season"] == "winter":
                enhanced_prompt += ", winter atmosphere, cold weather, winter setting"
            elif analysis_result["season"] == "spring":
                enhanced_prompt += ", spring atmosphere, fresh green, blooming flowers"
            elif analysis_result["season"] == "summer":
                enhanced_prompt += ", sunny summer day, bright sunlight, summer atmosphere"
        
        # å¼ºåŒ–åœºæ™¯æè¿°
        if analysis_result["scene_type"] == "park":
            enhanced_prompt += ", park setting with trees and benches"
        elif analysis_result["scene_type"] == "garden":
            enhanced_prompt += ", garden setting with flowers and plants"
        
        # å¼ºåŒ–äº’åŠ¨æè¿°
        if analysis_result["interaction"] == "sitting":
            enhanced_prompt += ", sitting together, seated position"
        elif analysis_result["interaction"] == "talking":
            enhanced_prompt += ", having a conversation, face to face interaction"
        
        return enhanced_prompt
    
    # æ‰§è¡Œåœºæ™¯åˆ†æ
    scene_analysis = analyze_prompt_intent(final_prompt)
    print(f"åœºæ™¯åˆ†æç»“æœ: {scene_analysis}".encode('utf-8', errors='replace').decode())
    
    # å¢å¼ºæç¤ºè¯
    enhanced_prompt = enhance_prompt_for_scene_analysis(final_prompt, scene_analysis)
    print(f"å¢å¼ºåçš„æç¤ºè¯: {enhanced_prompt}".encode('utf-8', errors='replace').decode())
    
    # ä¼˜åŒ–æç¤ºè¯ï¼šç¡®ä¿æ¨¡å‹ç†è§£éœ€è¦ä¸¤ä¸ªäººç‰©
    # å¦‚æœæç¤ºè¯ä¸­æåˆ°"å›¾ä¸€"å’Œ"å›¾äºŒ"ï¼Œå¼ºåŒ–è¿™ä¸ªä¿¡æ¯
    if "å›¾ä¸€" in enhanced_prompt or "å›¾1" in enhanced_prompt or "image 1" in enhanced_prompt.lower():
        if "å›¾äºŒ" in enhanced_prompt or "å›¾2" in enhanced_prompt or "image 2" in enhanced_prompt.lower():
            # æ˜ç¡®å¼ºè°ƒéœ€è¦ä¸¤ä¸ªäººç‰©ï¼Œå¹¶å¼ºè°ƒä¿ç•™åŸå›¾ç‰¹å¾
            enhanced_prompt = enhanced_prompt + ", two different people, two persons, two characters, keep facial features from reference images"

    # ç¡®å®šè¾“å‡ºå°ºå¯¸
    if output_width == 0 or output_height == 0:
        # ä½¿ç”¨å›¾1çš„å°ºå¯¸ä½œä¸ºåŸºå‡†
        orig_w, orig_h = image1.size
        aspect = orig_w / orig_h
        target_size = 1024
        if aspect > 1:
            target_w, target_h = target_size, max(512, int(target_size / aspect))
        else:
            target_h, target_w = target_size, max(512, int(target_size * aspect))
        target_w = (target_w // 16) * 16
        target_h = (target_h // 16) * 16
    else:
        target_w = (int(output_width) // 16) * 16
        target_h = (int(output_height) // 16) * 16

    # è°ƒæ•´ä¸¤å¼ å›¾åˆ°ç›¸åŒå°ºå¯¸
    image1_resized = image1.resize((target_w, target_h), Image.LANCZOS)
    image2_resized = image2.resize((target_w, target_h), Image.LANCZOS)

    if is_random: seed = random.randint(0, 2**32 - 1)
    generator = torch.Generator(DEVICE).manual_seed(int(seed))

    date_folder = datetime.now().strftime("%Y-%m-%d")
    save_dir = os.path.join(OUTPUT_ROOT, date_folder)
    os.makedirs(save_dir, exist_ok=True)

    results = []
    pipe = None
    
    try:
        pipe = manager.get_pipeline(t_choice, v_choice, selected_loras, weights_map, mode='img', perf_mode=perf_mode)
        
        # --- æ–°å¢ï¼šåº”ç”¨é‡‡æ ·å™¨ ---
        if sampler_name:
            new_scheduler = get_scheduler(sampler_name, pipe.scheduler.config)
            if new_scheduler is not None:
                print(f"Sampler set to: {sampler_name}")
                pipe.scheduler = new_scheduler

        import numpy as np

        for i in progress.tqdm(range(int(batch_size)), desc="æ™ºèƒ½èåˆç”Ÿæˆä¸­"):
            if is_interrupted: break
            torch.cuda.ipc_collect()
            
            # ä¸ºæ¯æ¬¡ç”Ÿæˆåˆ›å»ºæ–°çš„generatorï¼ˆå¦‚æœbatch_size > 1ï¼‰
            if i > 0:
                gen_seed = random.randint(0, 2**32 - 1) if is_random else seed + i
                current_generator = torch.Generator(DEVICE).manual_seed(int(gen_seed))
            else:
                current_generator = generator
            
            step_callback = make_progress_callback(progress, int(steps))

            # æ”¹è¿›ç­–ç•¥ï¼šå…ˆåŸºäºå›¾1ç”Ÿæˆåœºæ™¯ï¼Œç„¶ååŸºäºå›¾2æ·»åŠ ç¬¬äºŒä¸ªäººç‰©
            # è¿™æ ·å¯ä»¥æ›´å¥½åœ°æ§åˆ¶åœºæ™¯ç”Ÿæˆï¼Œé¿å…æ··ä¹±çš„å¤šé‡æ›å…‰æ•ˆæœ
            
            # ============ æ–°å¢ï¼šæ™ºèƒ½èåˆç­–ç•¥å¼•æ“ ============
            def select_fusion_strategy(scene_analysis, prompt_text):
                """
                æ ¹æ®åœºæ™¯åˆ†æå’Œæç¤ºè¯é€‰æ‹©æœ€é€‚åˆçš„èåˆç­–ç•¥
                """
                prompt_lower = prompt_text.lower()
                
                # æ£€æµ‹ä¸»ä½“ç±»å‹
                has_person_keywords = any(kw in prompt_lower for kw in ["äººç‰©", "person", "äºº", "man", "woman", "people", "character"])
                has_animal_keywords = any(kw in prompt_lower for kw in ["åŠ¨ç‰©", "animal", "çŒ«", "cat", "ç‹—", "dog", "å® ç‰©", "pet"])
                has_object_keywords = any(kw in prompt_lower for kw in ["ç‰©å“", "object", "è½¦", "car", "ä¸œè¥¿", "thing"])
                
                if has_person_keywords and scene_analysis["interaction"] in ["sitting", "talking", "standing"]:
                    # äººç‰©ç›¸å…³èåˆ
                    if "ååœ¨" in prompt_lower or "sitting" in prompt_lower:
                        return "person_sitting_scene"  # äººç‰©åå§¿åœºæ™¯
                    elif "èŠå¤©" in prompt_lower or "talking" in prompt_lower:
                        return "person_interaction"    # äººç‰©äº’åŠ¨
                    else:
                        return "person_scene"          # ä¸€èˆ¬äººç‰©åœºæ™¯
                elif has_animal_keywords or has_object_keywords:
                    # åŠ¨ç‰©/ç‰©å“ç›¸å…³èåˆ
                    if scene_analysis["interaction"] in ["playing", "holding"]:
                        return "interactive_object"    # äº’åŠ¨å‹ç‰©å“
                    else:
                        return "static_scene"          # é™æ€åœºæ™¯
                else:
                    return "general_fusion"            # ä¸€èˆ¬èåˆ
            
            fusion_strategy = select_fusion_strategy(scene_analysis, enhanced_prompt)
            print(f"é€‰æ‹©çš„èåˆç­–ç•¥: {fusion_strategy}".encode('utf-8', errors='replace').decode())
            
            # æ ¹æ®ç­–ç•¥è°ƒæ•´å‚æ•°
            def adjust_parameters_by_strategy(strategy, base_strength, base_cfg):
                """
                æ ¹æ®èåˆç­–ç•¥è°ƒæ•´ç”Ÿæˆå‚æ•°
                """
                strategy_params = {
                    "person_sitting_scene": {
                        "strength_multiplier": 1.1,  # ç¨é«˜çš„å˜åŒ–ï¼Œå…è®¸åœºæ™¯æ”¹å˜
                        "cfg_adjustment": 0.1,       # ç¨é«˜çš„CFG
                        "focus": "scene_building"    # é‡ç‚¹åœ¨åœºæ™¯æ„å»º
                    },
                    "person_interaction": {
                        "strength_multiplier": 0.9,  # ç¨ä½çš„å˜åŒ–ï¼Œä¿æŒäººç‰©ç‰¹å¾
                        "cfg_adjustment": 0.2,       # æ›´é«˜çš„CFGç¡®ä¿äº’åŠ¨æ•ˆæœ
                        "focus": "interaction_detail" # é‡ç‚¹åœ¨äº’åŠ¨ç»†èŠ‚
                    },
                    "person_scene": {
                        "strength_multiplier": 1.0,
                        "cfg_adjustment": 0.0,
                        "focus": "balanced"          # å¹³è¡¡å¤„ç†
                    },
                    "interactive_object": {
                        "strength_multiplier": 0.8,  # è¾ƒä½å˜åŒ–ï¼Œä¿æŒç‰©å“ç‰¹å¾
                        "cfg_adjustment": 0.1,
                        "focus": "object_preservation" # é‡ç‚¹åœ¨ç‰©å“ä¿æŒ
                    },
                    "static_scene": {
                        "strength_multiplier": 1.2,  # è¾ƒé«˜å˜åŒ–ï¼Œé‡å¡‘åœºæ™¯
                        "cfg_adjustment": -0.1,      # ç¨ä½çš„CFGé¿å…è¿‡åº¦çº¦æŸ
                        "focus": "scene_creation"    # é‡ç‚¹åœ¨åœºæ™¯åˆ›å»º
                    },
                    "general_fusion": {
                        "strength_multiplier": 1.0,
                        "cfg_adjustment": 0.0,
                        "focus": "balanced"
                    }
                }
                
                params = strategy_params.get(strategy, strategy_params["general_fusion"])
                
                adjusted_strength = min(0.9, max(0.3, base_strength * params["strength_multiplier"]))
                adjusted_cfg = max(0.5, min(2.0, base_cfg + params["cfg_adjustment"]))
                
                return adjusted_strength, adjusted_cfg, params["focus"]
            
            adjusted_strength, adjusted_cfg, fusion_focus = adjust_parameters_by_strategy(
                fusion_strategy, float(strength), float(cfg) if cfg > 0 else 1.0
            )
            
            print(f"èåˆç„¦ç‚¹: {fusion_focus}".encode('utf-8', errors='replace').decode())
            print(f"è°ƒæ•´åå‚æ•°: strength={adjusted_strength:.2f}, cfg={adjusted_cfg:.2f}".encode('utf-8', errors='replace').decode())
            
            # æ£€æµ‹æ˜¯å¦éœ€è¦ç¦»å¼€åŸåœºæ™¯
            has_new_scene = any(kw in enhanced_prompt.lower() for kw in ["åœ¨", "at", "in", "on", "åœºæ™¯", "scene", "èƒŒæ™¯", "background", 
                                                               "å…¬å›­", "park", "æ²³è¾¹", "river", "é•¿æ¤…", "bench"])
            
            # ============ æ–°å¢ï¼šæ™ºèƒ½èåˆæ‰§è¡Œå¼•æ“ ============
            def execute_intelligent_fusion(pipe, image1, image2, prompt, strategy, strength, cfg, generator, steps):
                """
                æ ¹æ®èåˆç­–ç•¥æ‰§è¡Œæ™ºèƒ½èåˆ
                """
                # ç¡®ä¿stepsæ˜¯æ•´æ•°
                steps = int(steps)
                
                if strategy == "person_sitting_scene":
                    # äººç‰©åå§¿åœºæ™¯ï¼šä¼˜å…ˆæ„å»ºåœºæ™¯ï¼Œç„¶åæ·»åŠ äººç‰©
                    print(f"æ‰§è¡Œäººç‰©åå§¿åœºæ™¯èåˆ...".encode('utf-8', errors='replace').decode())
                    
                    # æ­¥éª¤1: æ„å»ºåœºæ™¯åŸºç¡€
                    scene_prompt = prompt + ", establishing the scene, park bench, autumn setting, sitting area"
                    scene_result = pipe(
                        prompt=scene_prompt,
                        image=image1,
                        strength=strength * 0.6,  # è¾ƒä½å˜åŒ–ï¼Œæ„å»ºåœºæ™¯
                        num_inference_steps=max(8, int(steps * 0.6)),
                        guidance_scale=cfg,
                        generator=generator,
                        callback_on_step_end=None
                    ).images[0]
                    
                    # æ­¥éª¤2: æ·»åŠ ç¬¬ä¸€ä¸ªäººçš„ç‰¹å¾
                    person1_prompt = prompt + ", person from image 1 sitting, maintaining character features"
                    person1_result = pipe(
                        prompt=person1_prompt,
                        image=image1,
                        strength=strength * 0.4,  # å¾ˆä½å˜åŒ–ï¼Œä¿æŒäººç‰©ç‰¹å¾
                        num_inference_steps=max(6, int(steps * 0.4)),
                        guidance_scale=cfg + 0.1,
                        generator=generator,
                        callback_on_step_end=None
                    ).images[0]
                    
                    # æ­¥éª¤3: èåˆåœºæ™¯å’Œç¬¬ä¸€äºº
                    fusion1 = Image.blend(scene_result, person1_result, 0.3)
                    
                    # æ­¥éª¤4: æ·»åŠ ç¬¬äºŒä¸ªäººçš„ç‰¹å¾
                    person2_prompt = prompt + ", person from image 2 sitting nearby, maintaining character features"
                    person2_result = pipe(
                        prompt=person2_prompt,
                        image=image2,
                        strength=strength * 0.4,
                        num_inference_steps=max(6, int(steps * 0.4)),
                        guidance_scale=cfg + 0.1,
                        generator=generator,
                        callback_on_step_end=None
                    ).images[0]
                    
                    # æœ€ç»ˆèåˆ
                    final_result = Image.blend(fusion1, person2_result, 0.4)
                    return final_result
                    
                elif strategy == "person_interaction":
                    # äººç‰©äº’åŠ¨ï¼šé‡ç‚¹ä¿æŒäººç‰©ç‰¹å¾ï¼Œç¡®ä¿äº’åŠ¨è‡ªç„¶
                    print(f"æ‰§è¡Œäººç‰©äº’åŠ¨èåˆ...".encode('utf-8', errors='replace').decode())
                    
                    # æ­¥éª¤1: ä¿æŒå›¾1äººç‰©ç‰¹å¾ï¼Œæ·»åŠ åŸºç¡€äº’åŠ¨åœºæ™¯
                    interaction_base = pipe(
                        prompt=prompt + ", maintaining character features from image 1, establishing interaction setup",
                        image=image1,
                        strength=strength * 0.3,  # å¾ˆä½å˜åŒ–ï¼Œä¿æŒäººç‰©
                        num_inference_steps=max(6, int(steps * 0.5)),
                        guidance_scale=cfg + 0.2,  # é«˜CFGç¡®ä¿äº’åŠ¨æ•ˆæœ
                        generator=generator,
                        callback_on_step_end=None
                    ).images[0]
                    
                    # æ­¥éª¤2: èåˆç¬¬äºŒä¸ªäººç‰©çš„ç‰¹å¾
                    blend_factor = 0.5
                    arr_base = np.array(interaction_base).astype(np.float32) / 255.0
                    arr_img2 = np.array(image2).astype(np.float32) / 255.0
                    combined = arr_base * (1 - blend_factor) + arr_img2 * blend_factor
                    combined = np.clip(combined, 0, 1)
                    combined_pil = Image.fromarray((combined * 255).astype(np.uint8))
                    
                    # æ­¥éª¤3: ç²¾ç»†åŒ–äº’åŠ¨åœºæ™¯
                    final_result = pipe(
                        prompt=prompt + ", natural conversation, two people interacting, maintaining both character features",
                        image=combined_pil,
                        strength=strength * 0.4,
                        num_inference_steps=max(8, int(steps * 0.6)),
                        guidance_scale=cfg + 0.1,
                        generator=generator,
                        callback_on_step_end=None
                    ).images[0]
                    
                    return final_result
                    
                elif strategy == "interactive_object":
                    # äº’åŠ¨ç‰©å“ï¼šé‡ç‚¹ä¿æŒç‰©å“ç‰¹å¾
                    print(f"æ‰§è¡Œäº’åŠ¨ç‰©å“èåˆ...".encode('utf-8', errors='replace').decode())
                    
                    # ä¿æŒç‰©å“/åŠ¨ç‰©ç‰¹å¾ï¼Œæ·»åŠ äº’åŠ¨å…ƒç´ 
                    result = pipe(
                        prompt=prompt + ", maintaining object/animal features, natural interaction",
                        image=image1,
                        strength=strength * 0.5,
                        num_inference_steps=int(steps),
                        guidance_scale=cfg,
                        generator=generator,
                        callback_on_step_end=None
                    ).images[0]
                    
                    return result
                    
                else:
                    # ä¸€èˆ¬èåˆæˆ–é™æ€åœºæ™¯ï¼šä½¿ç”¨å¹³è¡¡ç­–ç•¥
                    print(f"æ‰§è¡Œä¸€èˆ¬èåˆ...".encode('utf-8', errors='replace').decode())
                    
                    # å¤šæ­¥éª¤èåˆ
                    step1_result = pipe(
                        prompt=prompt,
                        image=image1,
                        strength=strength * 0.6,
                        num_inference_steps=max(8, int(steps * 0.7)),
                        guidance_scale=cfg,
                        generator=generator,
                        callback_on_step_end=None
                    ).images[0]
                    
                    # èåˆç¬¬äºŒå¼ å›¾çš„ç‰¹å¾
                    blend_factor = 0.4
                    arr_step1 = np.array(step1_result).astype(np.float32) / 255.0
                    arr_img2 = np.array(image2).astype(np.float32) / 255.0
                    combined = arr_step1 * (1 - blend_factor) + arr_img2 * blend_factor
                    combined = np.clip(combined, 0, 1)
                    combined_pil = Image.fromarray((combined * 255).astype(np.uint8))
                    
                    final_result = pipe(
                        prompt=prompt,
                        image=combined_pil,
                        strength=strength * 0.5,
                        num_inference_steps=max(6, int(steps * 0.5)),
                        guidance_scale=cfg,
                        generator=generator,
                        callback_on_step_end=None
                    ).images[0]
                    
                    return final_result
            
            # æ‰§è¡Œæ™ºèƒ½èåˆ
            result = execute_intelligent_fusion(
                pipe, image1_resized, image2_resized, enhanced_prompt, 
                fusion_strategy, adjusted_strength, adjusted_cfg, 
                current_generator, int(steps)  # ç¡®ä¿ä¼ é€’æ•´æ•°
            )
            
            # ç»“æœåå¤„ç†å’Œä¼˜åŒ–
            print(f"æ™ºèƒ½èåˆå®Œæˆï¼Œä½¿ç”¨ç­–ç•¥: {fusion_strategy}".encode('utf-8', errors='replace').decode())
            
            # åº”ç”¨åå¤„ç†ä¼˜åŒ–
            output = result

            filename = f"fusion_{datetime.now().strftime('%H%M%S')}_{uuid.uuid4().hex[:4]}.png"
            path = os.path.join(save_dir, filename)
            output.save(path)
            results.append(output)  # è¿”å›PILå›¾åƒè€Œä¸æ˜¯è·¯å¾„ï¼Œä»¥ä¾¿åœ¨Galleryä¸­æ˜¾ç¤º

    except Exception as e:
        if "ä»»åŠ¡å·²æ‰‹åŠ¨åœæ­¢" in str(e):
            print("ä»»åŠ¡å·²åœæ­¢".encode('utf-8', errors='replace').decode())
        else:
            import traceback
            traceback.print_exc()
            raise gr.Error(f"èåˆç”Ÿæˆä¸­æ–­: {str(e)}")
    finally:
        if pipe:
            del pipe
        auto_flush_vram(vram_threshold)
        _, current_status = get_vram_info()

    return results, seed, current_status

def manual_force_flush():
    print("æ­£åœ¨å½»åº•æ¸…ç†æ˜¾å­˜ï¼Œæ­£åœ¨å¸è½½æ¨¡å‹...".encode('utf-8', errors='replace').decode())
    try:
        manager._clear_pipeline()
    except Exception as e:
        print(f"æ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°è­¦å‘Š: {e}".encode('utf-8', errors='replace').decode())
    gc.collect()
    torch.cuda.empty_cache()
    _, status = get_vram_info()
    print("æ˜¾å­˜å·²å½»åº•é‡Šæ”¾ã€‚".encode('utf-8', errors='replace').decode())
    return status

# ==========================================
# UI ç•Œé¢
# ==========================================
DEFAULT_PERF_MODE = "ä½ç«¯æœº (æ˜¾å­˜ä¼˜åŒ–)" if TOTAL_VRAM < 20 * 1024**3 else "é«˜ç«¯æœº (æ˜¾å­˜>=20GB)"
DEFAULT_LANG = "zh"

with gr.Blocks(title="Z-Image Pro Studio") as demo:
    CURRENT_LANG = gr.State(value=DEFAULT_LANG)
    T = I18N[DEFAULT_LANG]

    with gr.Row(elem_id="header_row"):
        lang_radio = gr.Radio(
            choices=["CN", "EN"], 
            value="CN", 
            scale=0, 
            container=False, 
            show_label=False,
            elem_id="lang_radio_selector"
        )
        title_md = gr.Markdown(value=T["title"], elem_id="app_title_markdown")
        
    with gr.Tabs() as tabs:
        # --- æ–‡æˆå›¾ ---
        with gr.Tab(label=T["tab_t2i"]) as tab_t2i:
            with gr.Row():
                with gr.Column(scale=4):
                    prompt_input = gr.Textbox(label=T["label_prompt"], lines=4)
                    manual_flush_btn = gr.Button(value=T["btn_flush_vram"], size="sm", variant="secondary")
                    vram_threshold_slider = gr.Slider(50, 98, 90, step=1, label=T["label_vram_threshold"])
                    
                    with gr.Accordion(label=T["acc_lora"], open=False) as t2i_lora_acc:
                        txt_lora_checks = []
                        txt_lora_sliders = []
                        
                        with gr.Row():
                            txt_refresh_lora_btn = gr.Button(value=T["btn_refresh_lora"], size="sm")
                            txt_lora_info_md = gr.Markdown("")
                        
                        if not LORA_FILES:
                            t2i_no_lora_md = gr.Markdown(value=T["txt_no_lora"])
                        else:
                            for fname in LORA_FILES:
                                with gr.Row():
                                    chk = gr.Checkbox(label=fname, value=False, scale=1, container=False)
                                    sld = gr.Slider(0, 2.0, 1.0, step=0.05, label=T["label_weight"], scale=4)
                                    txt_lora_checks.append(chk)
                                    txt_lora_sliders.append(sld)

                    with gr.Accordion(label=T["acc_model"], open=True) as t2i_model_acc:
                        refresh_models_btn = gr.Button(value=T["btn_refresh_model"], size="sm")
                        t_drop = gr.Dropdown(label=T["label_transformer"], choices=["default"] + scan_model_items(MOD_TRANS_DIR), value="default")
                        v_drop = gr.Dropdown(label=T["label_vae"], choices=["default"] + scan_model_items(MOD_VAE_DIR), value="default")
                        
                        perf_mode_radio = gr.Radio(
                            choices=[T["val_perf_high"], T["val_perf_low"]],
                            value=DEFAULT_PERF_MODE,
                            label=T["label_perf"]
                        )
                        
                        with gr.Row():
                            width_s = gr.Slider(512, 2048, 1024, step=16, label=T["label_width"])
                            height_s = gr.Slider(512, 2048, 1024, step=16, label=T["label_height"])
                        step_s = gr.Slider(1, 50, 8, label=T["label_steps"])
                        cfg_s = gr.Slider(0, 10, 0, label=T["label_cfg"])
                        batch_s = gr.Slider(1, 200, 1, step=1, label=T["label_batch"])
                        seed_n = gr.Number(label=T["label_seed"], value=-1, precision=0)
                        random_c = gr.Checkbox(label=T["label_random_seed"], value=True)
                        # æ–°å¢é‡‡æ ·å™¨ä¸‹æ‹‰æ¡†
                        t2i_sampler_drop = gr.Dropdown(label=T["label_sampler"], choices=SAMPLER_LIST, value="Default (Z-Image)")

                    with gr.Row():
                        run_btn = gr.Button(value=T["btn_run"], variant="primary", size="lg")
                        stop_btn = gr.Button(value=T["btn_stop"], variant="stop", size="lg", interactive=False)

                with gr.Column(scale=6):
                    res_gallery = gr.Gallery(label=T["label_output"], columns=2, height="80vh")
                    res_seed = gr.Number(label=T["label_seed"], interactive=False)
                    t2i_vram_info = gr.Markdown(value=T["msg_vram_loading"])

        # --- å›¾ç‰‡ç¼–è¾‘ ---
        with gr.Tab(label=T["tab_edit"]) as tab_edit:
            with gr.Row():
                with gr.Column():
                    image_input_path = gr.Image(label=T["label_upload_img"], type="filepath")
                    with gr.Group():
                        rotate_angle = gr.Slider(-360, 360, 0, step=1, label=T["label_rotate"])
                        crop_x = gr.Slider(0, 100, 0, step=1, label=T["label_crop_x"])
                        crop_y = gr.Slider(0, 100, 0, step=1, label=T["label_crop_y"])
                        crop_width = gr.Slider(0, 100, 100, step=1, label=T["label_crop_w"])
                        crop_height = gr.Slider(0, 100, 100, step=1, label=T["label_crop_h"])
                        flip_horizontal = gr.Checkbox(label=T["label_flip_h"])
                        flip_vertical = gr.Checkbox(label=T["label_flip_v"])
                    edit_btn = gr.Button(value=T["btn_edit"], variant="primary")
                with gr.Column():
                    edited_image_output = gr.Image(label=T["label_edited"], type="pil")
                    with gr.Group():
                        apply_filter = gr.Dropdown(
                            [T["f_blur"], T["f_contour"], T["f_detail"], T["f_edge"], T["f_edge_more"], 
                             T["f_emboss"], T["f_find_edge"], T["f_sharp"], T["f_smooth"], T["f_smooth_more"]], 
                            label=T["label_filter"]
                        )
                        brightness = gr.Slider(-100, 100, 0, step=1, label=T["label_brightness"])
                        contrast = gr.Slider(-100, 100, 0, step=1, label=T["label_contrast"])
                        saturation = gr.Slider(-100, 100, 0, step=1, label=T["label_saturation"])

            def edit_image_wrapper(image_path, angle, x, y, width, height, hflip, vflip, filter, brightness, contrast, saturation):
                if image_path is None: return None
                if isinstance(image_path, str):
                    image = Image.open(image_path)
                else:
                    image = image_path

                if angle != 0: image = image.rotate(angle, expand=True)
                if x or y or width < 100 or height < 100:
                    original_width, original_height = image.size
                    left = int(original_width * x / 100)
                    top = int(original_height * y / 100)
                    right = int(original_width * (x + width) / 100)
                    bottom = int(original_height * (y + height) / 100)
                    image = image.crop((left, top, right, bottom))
                if hflip: image = ImageOps.mirror(image)
                if vflip: image = ImageOps.flip(image)
                if filter:
                    filter_map_zh = {
                        "æ¨¡ç³Š": ImageFilter.BLUR, "è½®å»“": ImageFilter.CONTOUR, "ç»†èŠ‚": ImageFilter.DETAIL,
                        "è¾¹ç¼˜å¢å¼º": ImageFilter.EDGE_ENHANCE, "æ›´å¤šè¾¹ç¼˜å¢å¼º": ImageFilter.EDGE_ENHANCE_MORE,
                        "æµ®é›•": ImageFilter.EMBOSS, "æŸ¥æ‰¾è¾¹ç¼˜": ImageFilter.FIND_EDGES,
                        "é”åŒ–": ImageFilter.SHARPEN, "å¹³æ»‘": ImageFilter.SMOOTH, "æ›´å¤šå¹³æ»‘": ImageFilter.SMOOTH_MORE
                    }
                    filter_map_en = {
                        "Blur": ImageFilter.BLUR, "Contour": ImageFilter.CONTOUR, "Detail": ImageFilter.DETAIL,
                        "Edge Enhance": ImageFilter.EDGE_ENHANCE, "Edge Enhance More": ImageFilter.EDGE_ENHANCE_MORE,
                        "Emboss": ImageFilter.EMBOSS, "Find Edges": ImageFilter.FIND_EDGES,
                        "Sharpen": ImageFilter.SHARPEN, "Smooth": ImageFilter.SMOOTH, "Smooth More": ImageFilter.SMOOTH_MORE
                    }
                    fmap = filter_map_zh if filter in filter_map_zh else filter_map_en
                    filter_func = fmap.get(filter)
                    if filter_func: image = image.filter(filter_func)
                if brightness != 0:
                    enhancer = ImageEnhance.Brightness(image)
                    image = enhancer.enhance(1 + brightness / 100)
                if contrast != 0:
                    enhancer = ImageEnhance.Contrast(image)
                    image = enhancer.enhance(1 + contrast / 100)
                if saturation != 0:
                    enhancer = ImageEnhance.Color(image)
                    image = enhancer.enhance(1 + saturation / 100)
                return image

            edit_btn.click(
                fn=edit_image_wrapper,
                inputs=[image_input_path, rotate_angle, crop_x, crop_y, crop_width, crop_height, flip_horizontal, flip_vertical, apply_filter, brightness, contrast, saturation],
                outputs=edited_image_output
            )

        # --- å›¾ç”Ÿå›¾ UI ---
        with gr.Tab(label=T["tab_i2i"]) as tab_i2i:
            with gr.Row():
                with gr.Column(scale=4):
                    with gr.Group():
                        img2img_input_path = gr.Image(label=T["label_ref_img"], type="filepath")
                        img2img_prompt = gr.Textbox(label=T["label_prompt_rec"], lines=2, placeholder=T["ph_prompt_i2i"])
                        img2img_negative_prompt = gr.Textbox(
                            label=T.get("label_negative", "Negative Prompt"),
                            lines=2,
                            placeholder="low quality, blurry, bad anatomy"
                        )
                        img2img_flush = gr.Button(value=T["btn_flush_vram"], size="sm", variant="secondary")

                        with gr.Accordion(label=T["acc_lora"], open=False) as i2i_lora_acc:
                            with gr.Row():
                                i2i_refresh_lora_btn = gr.Button(value=T["btn_refresh_lora"], size="sm")
                            
                            if not LORA_FILES:
                                i2i_no_lora_md = gr.Markdown(value=T["txt_no_lora"])
                                img2img_lora_drop = gr.Dropdown(choices=[], visible=False)
                                img2img_lora_weight = gr.Slider(0.0, 1.0, 0.0, visible=False)
                            else:
                                img2img_lora_drop = gr.Dropdown(
                                    label="Img2Img LoRA",
                                    choices=["None"] + LORA_FILES,
                                    value="None"
                                )
                                img2img_lora_weight = gr.Slider(
                                    0.0, 1.0, 0.6,
                                    step=0.05,
                                    label="LoRA æƒé‡ï¼ˆImg2Imgï¼‰"
                                )

                    with gr.Accordion(label=T["acc_model"], open=True) as i2i_model_acc:
                        img2img_refresh_models = gr.Button(value=T["btn_refresh_model"], size="sm")
                        img2img_t_drop = gr.Dropdown(label=T["label_transformer"], choices=["default"] + scan_model_items(MOD_TRANS_DIR), value="default")
                        img2img_v_drop = gr.Dropdown(label=T["label_vae"], choices=["default"] + scan_model_items(MOD_VAE_DIR), value="default")
                        
                        img2img_perf_mode = gr.Radio(
                            choices=[T["val_perf_high"], T["val_perf_low"]],
                            value=DEFAULT_PERF_MODE,
                            label=T["label_perf"]
                        )
                        img2img_mode = gr.Radio(
                            choices=[
                                "A. ä¸¥æ ¼ä¿ç»“æ„ï¼ˆå¾®è°ƒé£æ ¼ï¼‰",
                                "B. å¼ºçƒˆå¬ promptï¼ˆå…è®¸å¤§æ”¹ï¼‰"
                            ],
                            value="A. ä¸¥æ ¼ä¿ç»“æ„ï¼ˆå¾®è°ƒé£æ ¼ï¼‰",
                            label="Img2Img æ¨¡å¼"
                        )
                        with gr.Row():
                            img2img_width_s = gr.Slider(0, 2048, 0, step=16, label=T["label_out_w"])
                            img2img_height_s = gr.Slider(0, 2048, 0, step=16, label=T["label_out_h"])
                        tip_md = gr.Markdown(value=T["tip_res"])
                        img2img_strength = gr.Slider(0.1, 0.9, 0.35, step=0.01,label=T["label_strength"])
                        img2img_steps = gr.Slider(1, 12, 6, step=1, label=T["label_steps"])
                        img2img_cfg = gr.Slider(0.5, 2.0, 1.0, step=0.05,label="CFG (Turbo Img2Img)")
                        img2img_batch = gr.Slider(1, 8, 1, step=1, label=T["label_batch"])
                        img2img_seed = gr.Number(label=T["label_seed"], value=42, precision=0)
                        img2img_random = gr.Checkbox(label=T["label_random_seed"], value=True)
                        # æ–°å¢é‡‡æ ·å™¨ä¸‹æ‹‰æ¡†
                        img2img_sampler_drop = gr.Dropdown(label=T["label_sampler"], choices=SAMPLER_LIST, value="Default (Z-Image)")
                    with gr.Row():
                        img2img_run_btn = gr.Button(value=T["btn_gen"], variant="primary", size="lg")
                        img2img_stop_btn = gr.Button(value=T["btn_stop_short"], variant="stop", size="lg", interactive=False)
                with gr.Column(scale=6):
                    img2img_gallery = gr.Gallery(label=T["label_gallery_i2i"], columns=2, height="80vh")
                    img2img_res_seed = gr.Number(label=T["label_seed"], interactive=False)
                    i2i_vram_info = gr.Markdown(value=T["msg_vram_loading"])

        # --- å±€éƒ¨é‡ç»˜ UI (Inpaint) ---
        with gr.Tab(label=T["tab_inpaint"]) as tab_inpaint:
            with gr.Row():
                with gr.Column(scale=4):
                    with gr.Group():
                        inpaint_input_img = gr.ImageEditor(
                            label="ä¸Šä¼ åŸå›¾å¹¶ç»˜åˆ¶è¦ä¿®æ”¹çš„åŒºåŸŸ",
                            type="pil"
                        )
                        with gr.Accordion(label="ğŸ“– å±€éƒ¨é‡ç»˜ä½¿ç”¨æŒ‡å—", open=False):
                            inpaint_tip_md = gr.Markdown(
                                value="## ğŸ¨ å±€éƒ¨é‡ç»˜ä½¿ç”¨æŒ‡å—\n\n" +
                                "### ğŸ“ æ“ä½œæ­¥éª¤ï¼š\n" +
                                "1. **ä¸Šä¼ åŸå›¾**ï¼šåœ¨å·¦ä¾§ä¸Šä¼ éœ€è¦ç¼–è¾‘çš„å›¾ç‰‡\n" +
                                "2. **ç»˜åˆ¶Mask**ï¼šç›´æ¥åœ¨å›¾ç‰‡ä¸Šä½¿ç”¨ç”»ç¬”ç»˜åˆ¶è¦ä¿®æ”¹çš„åŒºåŸŸ\n" +
                                "   - ğŸ¨ **æ¶‚æŠ¹åŒºåŸŸï¼ˆä»»ä½•é¢œè‰²ï¼‰**ï¼šå°†è¢«é‡æ–°ç”Ÿæˆ\n" +
                                "   - ğŸ–¤ **æœªç»˜åˆ¶åŒºåŸŸ**ï¼šä¿æŒåŸæ ·ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰\n" +
                                "3. **å¡«å†™æè¿°**ï¼šåœ¨Promptä¸­æè¿°æƒ³è¦çš„æ•ˆæœ\n" +
                                "4. **å¼€å§‹ç”Ÿæˆ**ï¼šç‚¹å‡»ç”ŸæˆæŒ‰é’®å¼€å§‹å±€éƒ¨é‡ç»˜\n\n" +
                                "### ğŸ¨ ç»˜åˆ¶æŠ€å·§ï¼š\n" +
                                "- å¯ä»¥ä½¿ç”¨**ä»»æ„é¢œè‰²çš„ç”»ç¬”**è¿›è¡Œæ¶‚æŠ¹ï¼ˆçº¢è‰²ã€è“è‰²ã€é»„è‰²ç­‰éƒ½å¯ä»¥ï¼‰\n" +
                                "- ç³»ç»Ÿä¼šè‡ªåŠ¨è¯†åˆ«æ‰€æœ‰æ¶‚æŠ¹åŒºåŸŸï¼Œæ— éœ€ä½¿ç”¨ç‰¹å®šé¢œè‰²\n" +
                                "- æœªç»˜åˆ¶çš„åŒºåŸŸå°†è‡ªåŠ¨ä¿æŒåŸæ ·ï¼ˆæ— éœ€æ¶‚æŠ¹é»‘è‰²ï¼‰\n" +
                                "- å¯ä»¥è°ƒæ•´ç”»ç¬”å¤§å°ä»¥ç²¾ç¡®æ§åˆ¶MaskèŒƒå›´\n\n" +
                                "### âš™ï¸ å‚æ•°è°ƒèŠ‚ï¼š\n" +
                                "- **é‡ç»˜å¼ºåº¦**ï¼šæ§åˆ¶ä¿®æ”¹çš„å¹…åº¦ (0.1-0.9)\n" +
                                "- **æ­¥æ•°**ï¼šå½±å“ç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦\n" +
                                "- **CFG**ï¼šæ§åˆ¶å¯¹Promptçš„éµå¾ªç¨‹åº¦\n\n" +
                                "### ğŸ’¡ ä½¿ç”¨æŠ€å·§ï¼š\n" +
                                "- åªéœ€ç»˜åˆ¶è¦ä¿®æ”¹çš„åŒºåŸŸï¼Œå…¶ä»–åŒºåŸŸè‡ªåŠ¨ä¿æŒåŸæ ·\n" +
                                "- æ¶‚æŠ¹åŒºåŸŸä¸è¦è¿‡å¤§ï¼Œå¦åˆ™æ•ˆæœä¸æ˜æ˜¾\n" +
                                "- å¯ä»¥å¤šæ¬¡å°è¯•ä¸åŒçš„å‚æ•°ç»„åˆ\n\n" +
                                "### âš ï¸ æ³¨æ„ï¼š\n" +
                                "- å¦‚æœæ²¡æœ‰ç»˜åˆ¶ä»»ä½•åŒºåŸŸï¼Œç³»ç»Ÿå°†è‡ªåŠ¨åˆ›å»ºä¸€ä¸ªç¤ºä¾‹Maskåœ¨å›¾ç‰‡ä¸­é—´"
                            )
                         
                        inpaint_flush = gr.Button(value=T["btn_flush_vram"], size="sm", variant="secondary")
                         
                        inpaint_prompt = gr.Textbox(label=T["label_prompt_rec"], lines=2, placeholder=T["ph_prompt_i2i"])
                        inpaint_negative_prompt = gr.Textbox(
                            label=T.get("label_negative", "Negative Prompt"),
                            lines=2,
                            placeholder="low quality, blurry, bad anatomy"
                        )

                        with gr.Accordion(label=T["acc_lora"], open=False) as inpaint_lora_acc:
                            with gr.Row():
                                inpaint_refresh_lora_btn = gr.Button(value=T["btn_refresh_lora"], size="sm")
                             
                            if not LORA_FILES:
                                inpaint_no_lora_md = gr.Markdown(value=T["txt_no_lora"])
                                inpaint_lora_drop = gr.Dropdown(choices=[], visible=False)
                                inpaint_lora_weight = gr.Slider(0.0, 1.0, 0.0, visible=False)
                            else:
                                inpaint_lora_drop = gr.Dropdown(
                                    label="Inpaint LoRA",
                                    choices=["None"] + LORA_FILES,
                                    value="None"
                                )
                                inpaint_lora_weight = gr.Slider(
                                    0.0, 1.0, 0.6,
                                    step=0.05,
                                    label="LoRA æƒé‡"
                                )

                    with gr.Accordion(label=T["acc_model"], open=True) as inpaint_model_acc:
                        inpaint_refresh_models = gr.Button(value=T["btn_refresh_model"], size="sm")
                        inpaint_t_drop = gr.Dropdown(label=T["label_transformer"], choices=["default"] + scan_model_items(MOD_TRANS_DIR), value="default")
                        inpaint_v_drop = gr.Dropdown(label=T["label_vae"], choices=["default"] + scan_model_items(MOD_VAE_DIR), value="default")
                         
                        inpaint_perf_mode = gr.Radio(
                            choices=[T["val_perf_high"], T["val_perf_low"]],
                            value=DEFAULT_PERF_MODE,
                            label=T["label_perf"]
                        )
                         
                        inpaint_strength = gr.Slider(0.1, 0.9, 0.6, step=0.01, label=T["label_strength"])
                        inpaint_steps = gr.Slider(1, 20, 8, step=1, label=T["label_steps"])
                        inpaint_cfg = gr.Slider(0.5, 2.0, 1.0, step=0.05, label=T["label_cfg_inpaint"])
                        inpaint_seed = gr.Number(label=T["label_seed"], value=42, precision=0)
                        inpaint_random = gr.Checkbox(label=T["label_random_seed"], value=True)
                        # æ–°å¢é‡‡æ ·å™¨ä¸‹æ‹‰æ¡†
                        inpaint_sampler_drop = gr.Dropdown(label=T["label_sampler"], choices=SAMPLER_LIST, value="Default (Z-Image)")
                         
                    with gr.Row():
                        inpaint_run_btn = gr.Button(value=T["btn_gen"], variant="primary", size="lg")
                        inpaint_stop_btn = gr.Button(value=T["btn_stop_short"], variant="stop", size="lg", interactive=False)

                with gr.Column(scale=6):
                    inpaint_gallery = gr.Gallery(label=T["label_gallery_inpaint"], columns=2, height="80vh")
                    inpaint_res_seed = gr.Number(label=T["label_seed"], interactive=False)
                    inpaint_vram_info = gr.Markdown(value=T["msg_vram_loading"])

        # --- èåˆå›¾ ---
        with gr.Tab(label=T["tab_fusion"]) as tab_fusion:
            desc_fusion_md = gr.Markdown(value=T["desc_fusion"])
            with gr.Row():
                with gr.Column(scale=4):
                    with gr.Group():
                        fusion_input1_path = gr.Image(label=T["label_img1"], type="filepath")
                        fusion_input2_path = gr.Image(label=T["label_img2"], type="filepath")
                        with gr.Accordion(label="ğŸ“– æ™ºèƒ½èåˆå›¾ä½¿ç”¨æŒ‡å—", open=False):
                            fusion_tip_md = gr.Markdown(
                                value="## ğŸ¨ æ™ºèƒ½èåˆå›¾ä½¿ç”¨æŒ‡å—\n\n" +
                                "### ğŸ“ åŠŸèƒ½è¯´æ˜ï¼š\n" +
                                "æ™ºèƒ½èåˆå›¾åŠŸèƒ½å¯ä»¥å°†ä¸¤å¼ å‚è€ƒå›¾çš„ç‰¹å¾èåˆï¼Œç”ŸæˆåŒ…å«ä¸¤å¼ å›¾å…ƒç´ çš„æ–°åœºæ™¯ã€‚\n\n" +
                                "### ğŸ¯ ä½¿ç”¨åœºæ™¯ï¼š\n" +
                                "#### 1ï¸âƒ£ **ä¸¤å¼ äººç‰©å›¾èåˆ**\n" +
                                "- **ç¤ºä¾‹**ï¼šä¸Šä¼ ä¸¤å¼ äººç‰©ç…§ç‰‡\n" +
                                "- **æç¤ºè¯ç¤ºä¾‹**ï¼š\"å›¾ä¸€çš„äººç‰©å’Œå›¾äºŒçš„äººç‰©ååœ¨å…¬å›­é•¿æ¤…ä¸ŠèŠå¤©\"\n" +
                                "- **æ•ˆæœ**ï¼šç”ŸæˆåŒ…å«å›¾ä¸€å’Œå›¾äºŒäººç‰©è„¸éƒ¨ç‰¹å¾çš„ä¸¤ä¸ªäººååœ¨é•¿æ¤…ä¸Šçš„åœºæ™¯\n" +
                                "- **æŠ€å·§**ï¼šå¯ä»¥åœ¨æç¤ºè¯ä¸­è¯¦ç»†æè¿°å›¾ä¸€äººç‰©çš„ç©¿ç€æ‰“æ‰®ã€å›¾äºŒäººç‰©çš„ç‰¹å¾ç­‰\n\n" +
                                "#### 2ï¸âƒ£ **äººç‰©+ç‰©å“/åŠ¨ç‰©äº’åŠ¨**\n" +
                                "- **ç¤ºä¾‹**ï¼šä¸Šä¼ ä¸€å¼ äººç‰©å›¾å’Œä¸€å¼ ç‰©å“/åŠ¨ç‰©å›¾\n" +
                                "- **æç¤ºè¯ç¤ºä¾‹**ï¼š\"å›¾ä¸€çš„äººç‰©æŠ±ç€å›¾äºŒçš„çŒ«å’ªåœ¨èŠ±å›­é‡Œç©è€\"\n" +
                                "- **æ•ˆæœ**ï¼šç”Ÿæˆäººç‰©å’Œç‰©å“/åŠ¨ç‰©äº’åŠ¨çš„åœºæ™¯\n" +
                                "- **æŠ€å·§**ï¼šè¯¦ç»†æè¿°äº’åŠ¨çš„æ–¹å¼å’Œåœºæ™¯ç¯å¢ƒ\n\n" +
                                "### âš™ï¸ å‚æ•°è¯´æ˜ï¼š\n" +
                                "- **èåˆæƒé‡**ï¼šæ§åˆ¶ä¸¤å¼ å›¾çš„èåˆæ¯”ä¾‹\n" +
                                "  - 0.0 = å®Œå…¨åå‘å›¾1çš„ç‰¹å¾\n" +
                                "  - 0.5 = å¹³è¡¡èåˆä¸¤å¼ å›¾\n" +
                                "  - 1.0 = å®Œå…¨åå‘å›¾2çš„ç‰¹å¾\n" +
                                "- **é‡ç»˜å¼ºåº¦**ï¼šæ§åˆ¶ç”Ÿæˆæ—¶çš„å˜åŒ–å¹…åº¦\n" +
                                "  - 0.5-0.7 = ä¿ç•™æ›´å¤šåŸå›¾ç‰¹å¾ï¼ˆæ¨èï¼‰\n" +
                                "  - 0.8-1.0 = æ›´å¤§å˜åŒ–ï¼Œæ›´ç¬¦åˆæç¤ºè¯\n" +
                                "- **æ­¥æ•°**ï¼šå½±å“ç”Ÿæˆè´¨é‡ï¼Œå»ºè®®15-30æ­¥\n" +
                                "- **CFG**ï¼šæ§åˆ¶å¯¹æç¤ºè¯çš„éµå¾ªç¨‹åº¦\n\n" +
                                "### ğŸ’¡ ä½¿ç”¨æŠ€å·§ï¼š\n" +
                                "1. **æç¤ºè¯è¦è¯¦ç»†**ï¼šæ˜ç¡®æè¿°ä¸¤å¼ å›¾å¦‚ä½•èåˆï¼ŒåŒ…æ‹¬åœºæ™¯ã€åŠ¨ä½œã€äº’åŠ¨æ–¹å¼ç­‰\n" +
                                "2. **èåˆæƒé‡è°ƒèŠ‚**ï¼šæ ¹æ®æƒ³è¦çš„æ•ˆæœè°ƒæ•´ï¼Œå¦‚æœæ›´æƒ³è¦å›¾1çš„ç‰¹å¾ï¼Œé™ä½æƒé‡ï¼›åä¹‹æé«˜æƒé‡\n" +
                                "3. **å¤šæ¬¡å°è¯•**ï¼šå¯ä»¥å°è¯•ä¸åŒçš„å‚æ•°ç»„åˆï¼Œæ‰¾åˆ°æœ€ä½³æ•ˆæœ\n" +
                                "4. **å›¾ç‰‡è´¨é‡**ï¼šä¸Šä¼ æ¸…æ™°ã€ä¸»ä½“æ˜ç¡®çš„å›¾ç‰‡æ•ˆæœæ›´å¥½\n\n" +
                                "### âš ï¸ æ³¨æ„äº‹é¡¹ï¼š\n" +
                                "- èåˆè¿‡ç¨‹éœ€è¦è¾ƒé•¿æ—¶é—´ï¼ˆéœ€è¦åˆ†åˆ«åŸºäºä¸¤å¼ å›¾ç”Ÿæˆï¼Œç„¶åèåˆï¼‰\n" +
                                "- å¦‚æœä¸¤å¼ å›¾å°ºå¯¸å·®å¼‚å¾ˆå¤§ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è°ƒæ•´åˆ°ç›¸åŒå°ºå¯¸\n" +
                                "- å»ºè®®ä½¿ç”¨æè¿°æ€§å¼ºçš„æç¤ºè¯ï¼Œæ•ˆæœä¼šæ›´å¥½"
                            )
                        fusion_prompt = gr.Textbox(label=T["label_fusion_prompt"], lines=3, placeholder="ä¾‹å¦‚ï¼šå›¾ä¸€çš„äººç‰©å’Œå›¾äºŒçš„äººç‰©ååœ¨å…¬å›­é•¿æ¤…ä¸ŠèŠå¤©ï¼Œé˜³å…‰æ˜åªšï¼ŒèƒŒæ™¯æ˜¯ç¾ä¸½çš„å…¬å›­")
                        fusion_flush = gr.Button(value=T["btn_flush_vram"], size="sm", variant="secondary")
                        
                        with gr.Accordion(label=T["acc_lora"], open=False) as fusion_lora_acc:
                            fusion_lora_checks = []
                            fusion_lora_sliders = []
                            
                            with gr.Row():
                                fusion_refresh_lora_btn = gr.Button(value=T["btn_refresh_lora"], size="sm")
                                fusion_lora_info_md = gr.Markdown("")

                            if not LORA_FILES:
                                fusion_no_lora_md = gr.Markdown(value=T["txt_no_lora"])
                            else:
                                for fname in LORA_FILES:
                                    with gr.Row():
                                        chk = gr.Checkbox(label=fname, value=False, scale=1, container=False)
                                        sld = gr.Slider(0, 2.0, 1.0, step=0.05, label=T["label_weight"], scale=4)
                                        fusion_lora_checks.append(chk)
                                        fusion_lora_sliders.append(sld)

                    with gr.Accordion(label=T["acc_model"], open=True) as fusion_model_acc:
                        fusion_refresh_models = gr.Button(value=T["btn_refresh_model"], size="sm")
                        fusion_t_drop = gr.Dropdown(label=T["label_transformer"], choices=["default"] + scan_model_items(MOD_TRANS_DIR), value="default")
                        fusion_v_drop = gr.Dropdown(label=T["label_vae"], choices=["default"] + scan_model_items(MOD_VAE_DIR), value="default")
                        
                        fusion_perf_mode = gr.Radio(
                            choices=[T["val_perf_high"], T["val_perf_low"]],
                            value=DEFAULT_PERF_MODE,
                            label=T["label_perf"]
                        )
                        
                        with gr.Row():
                            fusion_width_s = gr.Slider(0, 2048, 0, step=16, label=T["label_out_w"])
                            fusion_height_s = gr.Slider(0, 2048, 0, step=16, label=T["label_out_h"])
                        fusion_tip_md = gr.Markdown(value=T["tip_res"])
                        with gr.Row():
                            fusion_blend = gr.Slider(0.0, 1.0, 0.5, step=0.05, label=T["label_blend"])
                            fusion_strength = gr.Slider(0.0, 1.0, 0.5, step=0.05, label=T["label_denoise"], info="å»ºè®®0.4-0.6ï¼Œæ•°å€¼è¶Šä½è¶Šä¿ç•™åŸå›¾ç‰¹å¾")
                        fusion_steps = gr.Slider(1, 100, 15, step=1, label=T["label_steps"])
                        fusion_cfg = gr.Slider(0.0, 2.0, 1.0, step=0.05, label="CFG (æ§åˆ¶å¯¹æç¤ºè¯çš„éµå¾ªç¨‹åº¦)")
                        fusion_batch = gr.Slider(1, 8, 1, step=1, label=T["label_batch"])
                        fusion_seed = gr.Number(label=T["label_seed"], value=42, precision=0)
                        fusion_random = gr.Checkbox(label=T["label_random_seed"], value=True)
                        # æ–°å¢é‡‡æ ·å™¨ä¸‹æ‹‰æ¡†
                        fusion_sampler_drop = gr.Dropdown(label=T["label_sampler"], choices=SAMPLER_LIST, value="Default (Z-Image)")
                    with gr.Row():
                        fusion_run_btn = gr.Button(value=T["btn_run"], variant="primary", size="lg")
                        fusion_stop_btn = gr.Button(value=T["btn_stop"], variant="stop", size="lg", interactive=False)
                with gr.Column(scale=6):
                    fusion_gallery = gr.Gallery(label=T["label_gallery_fusion"], columns=2, height="80vh")
                    fusion_res_seed = gr.Number(label=T["label_seed"], interactive=False)

    # -----------------------
    # è¯­è¨€åˆ‡æ¢é€»è¾‘
    # -----------------------
    def change_language(lang_choice):
        lang_code = "zh" if "ä¸­æ–‡" in lang_choice else "en"
        t = I18N.get(lang_code, I18N["zh"])
        return (
            lang_code,                       
            gr.update(value=t["title"]),                      
            gr.update(label=t["tab_t2i"]),   
            gr.update(label=t["tab_edit"]),  
            gr.update(label=t["tab_i2i"]),   
            gr.update(label=t["tab_inpaint"]),
            gr.update(label=t["tab_fusion"]), 
            
            gr.update(label=t["label_prompt"]),   
            t["btn_flush_vram"],                  
            gr.update(label=t["label_vram_threshold"]), 
            
            gr.update(label=t["acc_lora"]),        
            t["btn_refresh_lora"],                 
            gr.update(label=t["acc_model"]),       
            t["btn_refresh_model"],                
            gr.update(label=t["label_transformer"]), 
            gr.update(label=t["label_vae"]),       
            gr.update(label=t["label_perf"]), 
            
            gr.update(label=t["label_width"]),     
            gr.update(label=t["label_height"]),    
            gr.update(label=t["label_steps"]),       
            gr.update(label=t["label_cfg"]),         
            gr.update(label=t["label_batch"]),      
            gr.update(label=t["label_seed"]),        
            gr.update(label=t["label_random_seed"]), 
            
            t["btn_run"],                          
            t["btn_stop"],                         
            gr.update(label=t["label_output"]),     
            gr.update(label=t["label_seed"]),       
            
            gr.update(label=t["label_upload_img"]),  
            gr.update(label=t["label_rotate"]),     
            gr.update(label=t["label_crop_x"]),      
            gr.update(label=t["label_crop_y"]),      
            gr.update(label=t["label_crop_w"]),      
            gr.update(label=t["label_crop_h"]),      
            gr.update(label=t["label_flip_h"]),      
            gr.update(label=t["label_flip_v"]),      
            t["btn_edit"],                           
            gr.update(label=t["label_edited"]),      
            gr.update(choices=[t["f_blur"], t["f_contour"], t["f_detail"], t["f_edge"], t["f_edge_more"], 
                             t["f_emboss"], t["f_find_edge"], t["f_sharp"], t["f_smooth"], t["f_smooth_more"]], label=t["label_filter"]), 
            gr.update(label=t["label_brightness"]),  
            gr.update(label=t["label_contrast"]),    
            gr.update(label=t["label_saturation"]),  
            
            gr.update(label=t["label_ref_img"]),     
            gr.update(label=t["label_prompt_rec"], placeholder=t["ph_prompt_i2i"]), 
            t["btn_flush_vram"],                     
            gr.update(label=t["acc_lora"]),         
            t["btn_refresh_lora"],                  
            gr.update(label=t["acc_model"]),         
            t["btn_refresh_model"],                 
            gr.update(label=t["label_transformer"]), 
            gr.update(label=t["label_vae"]),        
            gr.update(label=t["label_perf"]), 
            
            gr.update(label=t["label_out_w"]),      
            gr.update(label=t["label_out_h"]),      
            t["tip_res"],                           
            gr.update(label=t["label_strength"]),    
            gr.update(label=t["label_steps"]),       
            gr.update(label=t["label_cfg_turbo"]),   
            gr.update(label=t["label_batch"]),      
            gr.update(label=t["label_seed"]),       
            gr.update(label=t["label_random_seed"]), 
            
            t["btn_gen"],                           
            t["btn_stop_short"],                    
            gr.update(label=t["label_gallery_i2i"]), 
            gr.update(label=t["label_seed"]),       

            # Inpaint æ›´æ–°
            gr.update(label="ä¸Šä¼ åŸå›¾å¹¶ç»˜åˆ¶è¦ä¿®æ”¹çš„åŒºåŸŸ"),  # æ›´æ–°ä¸ºæ–°çš„æ ‡ç­¾
            t["lbl_inpaint_tip"],
            gr.update(label=t["label_prompt_rec"], placeholder=t["ph_prompt_i2i"]),
            t["btn_flush_vram"],
            gr.update(label=t["acc_lora"]),
            t["btn_refresh_lora"],
            gr.update(label=t["acc_model"]),
            t["btn_refresh_model"],
            gr.update(label=t["label_transformer"]),
            gr.update(label=t["label_vae"]),
            gr.update(label=t["label_perf"]),
            gr.update(label=t["label_strength"]),
            gr.update(label=t["label_steps"]),
            gr.update(label=t["label_cfg_inpaint"]),
            gr.update(label=t["label_seed"]),
            gr.update(label=t["label_random_seed"]),
            t["btn_gen"],
            t["btn_stop_short"],
            gr.update(label=t["label_gallery_inpaint"]),
            gr.update(label=t["label_seed"]),
            
            t["desc_fusion"],                       
            gr.update(label=t["label_img1"]),       
            gr.update(label=t["label_img2"]),       
            gr.update(label=t["label_fusion_prompt"]), 
            t["btn_flush_vram"],                    
            gr.update(label=t["acc_lora"]),         
            t["btn_refresh_lora"],                  
            gr.update(label=t["acc_model"]),         
            t["btn_refresh_model"],                 
            gr.update(label=t["label_transformer"]), 
            gr.update(label=t["label_vae"]),         
            gr.update(label=t["label_perf"]), 
            
            gr.update(label=t["label_out_w"]),      
            gr.update(label=t["label_out_h"]),      
            t["tip_res"],                           
            gr.update(label=t["label_blend"]),       
            gr.update(label=t["label_denoise"]),    
            gr.update(label=t["label_steps"]),       
            gr.update(label=t["label_cfg_fixed"]),   
            gr.update(label=t["label_batch"]),      
            gr.update(label=t["label_seed"]),       
            gr.update(label=t["label_random_seed"]), 
            
            t["btn_run"],                          
            t["btn_stop"],                         
            gr.update(label=t["label_gallery_fusion"]), 
            gr.update(label=t["label_seed"]),       
        )

    lang_outputs = [
        CURRENT_LANG,
        title_md, 
        tab_t2i, tab_edit, tab_i2i, tab_inpaint, tab_fusion,
        prompt_input, manual_flush_btn, vram_threshold_slider,
        t2i_lora_acc, txt_refresh_lora_btn, t2i_model_acc, refresh_models_btn,
        t_drop, v_drop, perf_mode_radio,
        width_s, height_s, step_s, cfg_s, batch_s, seed_n, random_c,
        run_btn, stop_btn, res_gallery, res_seed,
        image_input_path, rotate_angle, crop_x, crop_y, crop_width, crop_height, flip_horizontal, flip_vertical,
        edit_btn, edited_image_output, apply_filter, brightness, contrast, saturation,
        img2img_input_path, img2img_prompt, img2img_flush,
        i2i_lora_acc, i2i_refresh_lora_btn, i2i_model_acc, img2img_refresh_models,
        img2img_t_drop, img2img_v_drop, img2img_perf_mode,
        img2img_width_s, img2img_height_s, tip_md, img2img_strength, img2img_steps,
        img2img_cfg, img2img_batch, img2img_seed, img2img_random,
        img2img_run_btn, img2img_stop_btn, img2img_gallery, img2img_res_seed,
        # Inpaint Outputs
        inpaint_input_img, inpaint_tip_md, inpaint_prompt, inpaint_flush, inpaint_lora_acc, inpaint_refresh_lora_btn,
        inpaint_model_acc, inpaint_refresh_models, inpaint_t_drop, inpaint_v_drop, inpaint_perf_mode,
        inpaint_strength, inpaint_steps, inpaint_cfg, inpaint_seed, inpaint_random,
        inpaint_run_btn, inpaint_stop_btn, inpaint_gallery, inpaint_res_seed,
        # Fusion Outputs
        desc_fusion_md, fusion_input1_path, fusion_input2_path, fusion_prompt, fusion_flush,
        fusion_lora_acc, fusion_refresh_lora_btn, fusion_model_acc, fusion_refresh_models,
        fusion_t_drop, fusion_v_drop, fusion_perf_mode,
        fusion_width_s, fusion_height_s, fusion_tip_md, fusion_blend, fusion_strength,
        fusion_steps, fusion_cfg, fusion_batch, fusion_seed, fusion_random,
        fusion_run_btn, fusion_stop_btn, fusion_gallery, fusion_res_seed
    ]

    def ui_to_running():
        return gr.update(interactive=False), gr.update(interactive=True)

    def ui_to_idle():
        return gr.update(interactive=True), gr.update(interactive=False)

    def trigger_interrupt():
        global is_interrupted
        is_interrupted = True
        return "æ­£åœ¨å¼ºåˆ¶ä¸­æ–­..."

    lang_radio.change(
        fn=change_language,
        inputs=lang_radio,
        outputs=lang_outputs
    )
    
    # -----------------------
    # åˆ·æ–°æ¨¡å‹å‡½æ•° (ä¿®å¤è¯­æ³•é”™è¯¯)
    # -----------------------
    def refresh_models_list():
        return (
            gr.update(choices=["default"] + scan_model_items(MOD_TRANS_DIR)),
            gr.update(choices=["default"] + scan_model_items(MOD_VAE_DIR))
        )

    refresh_models_btn.click(
        fn=refresh_models_list,
        outputs=[t_drop, v_drop]
    )
    
    txt_refresh_lora_btn.click(fn=lambda l: refresh_lora_list(l), inputs=CURRENT_LANG, outputs=txt_lora_info_md)
    
    manual_flush_btn.click(
        fn=manual_force_flush,
        outputs=t2i_vram_info
    )

    txt_ui_inputs = [prompt_input] + txt_lora_checks + txt_lora_sliders
    for c in txt_lora_checks + txt_lora_sliders:
        c.change(fn=update_prompt_ui_base, inputs=txt_ui_inputs, outputs=prompt_input)

    # ------------------------
    # æ–‡ç”Ÿå›¾ï¼ˆText2Imageï¼‰
    # ------------------------
    inference_event = run_btn.click(
        fn=ui_to_running,  # åªæ›´æ–°æŒ‰é’®çŠ¶æ€
        outputs=[run_btn, stop_btn]
    ).then(
        fn=run_inference,  # è¿”å›æœ€ç»ˆå›¾åƒ + ç§å­ + VRAMä¿¡æ¯
        # ä¼ å…¥é‡‡æ ·å™¨å‚æ•° t2i_sampler_drop
        inputs=txt_ui_inputs + [t_drop, v_drop, perf_mode_radio, width_s, height_s, step_s, cfg_s, seed_n, random_c, batch_s, vram_threshold_slider, t2i_sampler_drop],
        outputs=[res_gallery, res_seed, t2i_vram_info]
    ).then(
        fn=ui_to_idle,
        outputs=[run_btn, stop_btn]
    )

    stop_btn.click(
        fn=trigger_interrupt,
        outputs=t2i_vram_info
    ).then(
        fn=ui_to_idle,
        outputs=[run_btn, stop_btn],
        cancels=[inference_event]
    )
    
    def refresh_all_models_img():
        return (
            gr.update(choices=["default"] + scan_model_items(MOD_TRANS_DIR)),
            gr.update(choices=["default"] + scan_model_items(MOD_VAE_DIR))
        )

    img2img_refresh_models.click(fn=refresh_all_models_img, outputs=[img2img_t_drop, img2img_v_drop])
    
    img2img_flush.click(
        fn=manual_force_flush,
        outputs=i2i_vram_info
    )

    img2img_event = img2img_run_btn.click(
        fn=ui_to_running,
        outputs=[img2img_run_btn, img2img_stop_btn]
    ).then(
        fn=run_img2img,
        inputs=[
            img2img_prompt,
            img2img_negative_prompt,
            img2img_input_path, # ä½¿ç”¨ path
            img2img_width_s,
            img2img_height_s,
            img2img_steps,
            img2img_cfg,
            img2img_strength,
            img2img_seed,
            img2img_t_drop,
            img2img_v_drop,
            img2img_perf_mode,
            img2img_lora_drop,
            img2img_lora_weight,
            img2img_mode,
            img2img_sampler_drop  # ä¼ å…¥é‡‡æ ·å™¨
        ],
        outputs=[
            img2img_gallery,
            img2img_res_seed,
            i2i_vram_info
        ]
    ).then(
        fn=ui_to_idle,
        outputs=[img2img_run_btn, img2img_stop_btn]
    )

    img2img_stop_btn.click(
        fn=trigger_interrupt, 
        outputs=i2i_vram_info
    ).then(
        fn=ui_to_idle, 
        outputs=[img2img_run_btn, img2img_stop_btn], 
        cancels=[img2img_event]
    )

    # --- å±€éƒ¨é‡ç»˜ äº‹ä»¶ç»‘å®š ---
    def refresh_all_models_inpaint():
        return (
            gr.update(choices=["default"] + scan_model_items(MOD_TRANS_DIR)),
            gr.update(choices=["default"] + scan_model_items(MOD_VAE_DIR))
        )

    inpaint_refresh_models.click(fn=refresh_all_models_inpaint, outputs=[inpaint_t_drop, inpaint_v_drop])
    
    inpaint_flush.click(
        fn=manual_force_flush,
        outputs=inpaint_vram_info
    )

    inpaint_event = inpaint_run_btn.click(
        fn=ui_to_running,
        outputs=[inpaint_run_btn, inpaint_stop_btn]
    ).then(
        fn=run_inpainting,
        inputs=[
            inpaint_input_img,  # ImageEditoræ•°æ®
            inpaint_prompt,
            inpaint_negative_prompt,
            inpaint_steps,
            inpaint_cfg,
            inpaint_strength,
            inpaint_seed,
            inpaint_t_drop,
            inpaint_v_drop,
            inpaint_perf_mode,
            inpaint_lora_drop,
            inpaint_lora_weight,
            inpaint_sampler_drop  # ä¼ å…¥é‡‡æ ·å™¨
        ],
        outputs=[
            inpaint_gallery,
            inpaint_res_seed,
            inpaint_vram_info
        ]
    ).then(
        fn=ui_to_idle,
        outputs=[inpaint_run_btn, inpaint_stop_btn]
    )

    inpaint_stop_btn.click(
        fn=trigger_interrupt, 
        outputs=inpaint_vram_info
    ).then(
        fn=ui_to_idle, 
        outputs=[inpaint_run_btn, inpaint_stop_btn], 
        cancels=[inpaint_event]
    )

    # --- èåˆå›¾ ---
    fusion_refresh_models.click(fn=refresh_all_models_img, outputs=[fusion_t_drop, fusion_v_drop])
    
    fusion_refresh_lora_btn.click(fn=lambda l: refresh_lora_list(l), inputs=CURRENT_LANG, outputs=fusion_lora_info_md)
    
    fusion_flush.click(
        fn=manual_force_flush,
        outputs=i2i_vram_info
    )

    fusion_ui_inputs = [fusion_prompt] + fusion_lora_checks + fusion_lora_sliders
    for c in fusion_lora_checks + fusion_lora_sliders:
        c.change(fn=update_prompt_ui_base, inputs=fusion_ui_inputs, outputs=fusion_prompt)

    fusion_event = fusion_run_btn.click(
        fn=ui_to_running, 
        outputs=[fusion_run_btn, fusion_stop_btn]
    ).then(
        fn=run_fusion_img,
        inputs=[fusion_input1_path, fusion_input2_path, fusion_prompt] + fusion_lora_checks + fusion_lora_sliders + 
                [fusion_t_drop, fusion_v_drop, fusion_perf_mode, fusion_width_s, fusion_height_s,
                 fusion_blend, fusion_strength, fusion_steps, fusion_cfg, 
                 fusion_seed, fusion_random, fusion_batch, vram_threshold_slider, fusion_sampler_drop],  # ä¼ å…¥é‡‡æ ·å™¨
        outputs=[fusion_gallery, fusion_res_seed, i2i_vram_info]
    ).then(
        fn=ui_to_idle, 
        outputs=[fusion_run_btn, fusion_stop_btn]
    )

    fusion_stop_btn.click(
        fn=trigger_interrupt, 
        outputs=i2i_vram_info
    ).then(
        fn=ui_to_idle, 
        outputs=[fusion_run_btn, fusion_stop_btn], 
        cancels=[fusion_event]
    )


if __name__ == "__main__":
    demo.launch(share=False, server_name="127.0.0.1", server_port=7860, inbrowser=False)